<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="深度学习入门-基于python的理论与实现, HTML,CSS,React,Vue,Java,Go,微服务,linux">
    <meta name="description" content="1. python 入门1.5 Numpy
深度学习中经常出现数组和矩阵运算，Numpy 的数组类 numpy.array 提供了很多便捷的方法

1.5.1 导入 Numpyimport numpy as np

1.5.2 生成 Num">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>深度学习入门-基于python的理论与实现 | malred-blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
<meta name="generator" content="Hexo 5.4.2">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="container">
            <div class="nav-wrapper">
                <div class="brand-logo">
                    <a href="/" class="waves-effect waves-light">
                        
                        <img src="/medias/logo.png" class="logo-img hide-on-small-only">
                        
                        <span class="logo-span">malred-blog</span>
                    </a>
                </div>
                

<a href="#" data-activates="mobile-nav" class="button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li>
        <a id="toggleSearch" class="waves-effect waves-light">
            <i id="searchIcon" class="mdi-action-search" title="搜索"></i>
        </a>
    </li>

</ul>

<div class="side-nav" id="mobile-nav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">malred-blog</div>
        <div class="logo-desc">
            
            前后端学习者,存放知识笔记
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                友情链接
            </a>
        </li>
        
        
    </ul>

    <div class="social-link">
    <a href="https://github.com/malred" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:malguy2022@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2725953379" class="tooltipped" data-tooltip="QQ联系我: 2725953379" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>


</div>
</div>

            </div>
        </div>

        
    </nav>
</header>


<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover post-cover" style="background-image: url('/medias/featureimages/3.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        深度学习入门-基于python的理论与实现
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }
</style>
<div class="row">
    <div class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/ai/" target="_blank">
                                <span class="chip bg-color">ai</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/ai/" class="post-category" target="_blank">
                                ai
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-08-26
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>文章字数:&nbsp;&nbsp;
                        26.4k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>阅读时长:&nbsp;&nbsp;
                        105 分
                    </div>
                    
                
				
				
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="fa fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="1-python-入门"><a href="#1-python-入门" class="headerlink" title="1. python 入门"></a>1. python 入门</h1><h2 id="1-5-Numpy"><a href="#1-5-Numpy" class="headerlink" title="1.5 Numpy"></a>1.5 Numpy</h2><blockquote>
<p>深度学习中经常出现数组和矩阵运算，Numpy 的数组类 numpy.array 提供了很多便捷的方法</p>
</blockquote>
<h3 id="1-5-1-导入-Numpy"><a href="#1-5-1-导入-Numpy" class="headerlink" title="1.5.1 导入 Numpy"></a>1.5.1 导入 Numpy</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
</code></pre>
<h3 id="1-5-2-生成-Numpy-数组"><a href="#1-5-2-生成-Numpy-数组" class="headerlink" title="1.5.2 生成 Numpy 数组"></a>1.5.2 生成 Numpy 数组</h3><blockquote>
<p>np.array()，接收 python 列表，生成 Numpy 数组</p>
</blockquote>
<pre class=" language-python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>type<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<h3 id="1-5-3-Numpy-的数学运算"><a href="#1-5-3-Numpy-的数学运算" class="headerlink" title="1.5.3 Numpy 的数学运算"></a>1.5.3 Numpy 的数学运算</h3><pre class=" language-python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">,</span> <span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">-</span> y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">*</span> y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">/</span> y<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>需要注意，用于计算的数组的元素个数要相同</p>
</blockquote>
<blockquote>
<p>‘对应元素的’的英文是 element-wise，而 numpy 不仅可以进行 element-wise 运算，还可以和单一的数值（标量）组合起来进行计算。此时，需要在 numpy 数组的各个元素和标量之间进行计算，这个功能也称为广播</p>
</blockquote>
<pre class=" language-python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">/</span> <span class="token number">2.0</span><span class="token punctuation">)</span>
</code></pre>
<h3 id="1-5-4-Numpy-的-N-维数组"><a href="#1-5-4-Numpy-的-N-维数组" class="headerlink" title="1.5.4 Numpy 的 N 维数组"></a>1.5.4 Numpy 的 N 维数组</h3><blockquote>
<p>numpy 可以生成多维数组</p>
</blockquote>
<pre class=" language-python"><code class="language-python">A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>shape 可以查看矩阵的形状，dtype 可以查看矩阵元素的数据类型</p>
</blockquote>
<pre class=" language-python"><code class="language-python">B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A <span class="token operator">+</span> B<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A <span class="token operator">*</span> B<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>矩阵运算可以在相同形状的矩阵间以对应元素的方式进行。也可以通过标量（单一数值）对矩阵进行算术运算。这也是基于广播的功能</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>数学上将一维数组称为‘向量’，将二维数组称为‘矩阵’，将一般化后的向量或矩阵等统称为‘张量’（tensor）。本书将二维数组称为矩阵，三维及以上称为‘张量’或‘多维数组’</p>
</blockquote>
<h3 id="1-5-5-广播"><a href="#1-5-5-广播" class="headerlink" title="1.5.5 广播"></a>1.5.5 广播</h3><blockquote>
<p>numpy 中形状不同的数组之间也可以进行运算。之前的例子中，2x2 的矩阵和标量 10 之间进行了乘法运算。这个过程中，标量 10 被扩展成 2x2 的形状，然后再与矩阵 A 进行乘法运算。这个功能就是广播</p>
</blockquote>
<p><img src="20230826063229.png"><br><img src="20230826063607.png"></p>
<h3 id="1-5-6-访问元素"><a href="#1-5-6-访问元素" class="headerlink" title="1.5.6 访问元素"></a>1.5.6 访问元素</h3><pre class=" language-python"><code class="language-python">X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">51</span><span class="token punctuation">,</span> <span class="token number">55</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 第0行</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (0,1)的元素</span>
</code></pre>
<blockquote>
<p>使用 for 遍历</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> row <span class="token keyword">in</span> X<span class="token punctuation">:</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>row<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>使用数组访问</p>
</blockquote>
<pre class=" language-python"><code class="language-python">X <span class="token operator">=</span> X<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 将X转为一维数组</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">[</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 获取索引为0、2、4的元素</span>
</code></pre>
<blockquote>
<p>通过这个标记法，可以获取满足一定条件的元素。例如，获取 x 中大于 15 的元素</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>X <span class="token operator">></span> <span class="token number">15</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">[</span>X <span class="token operator">></span> <span class="token number">15</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>通过不等号得到了布尔型数组，并通过布尔型数组取出 X 的各个元素（取出 True 对应的元素）</p>
</blockquote>
<ul>
<li>python 等动态语言一般比 c 和 c++等静态语言（编译型）运算速度慢，所以很多追求性能的场景，人们用 c、c++编写，然后让 python 调用，numpy 也是如此</li>
</ul>
<h2 id="1-6-Matplotlib"><a href="#1-6-Matplotlib" class="headerlink" title="1.6 Matplotlib"></a>1.6 Matplotlib</h2><blockquote>
<p>图形绘制和可视化的库</p>
</blockquote>
<h3 id="1-6-1-绘制简单图形"><a href="#1-6-1-绘制简单图形" class="headerlink" title="1.6.1 绘制简单图形"></a>1.6.1 绘制简单图形</h3><blockquote>
<p>sin 函数曲线</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment" spellcheck="true"># 生成数据</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 以0.1为步长（单位），生成0到6的数据</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 绘制图形</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="20230826064850.png"></p>
<h3 id="1-6-2-pyplot-的功能"><a href="#1-6-2-pyplot-的功能" class="headerlink" title="1.6.2 pyplot 的功能"></a>1.6.2 pyplot 的功能</h3><blockquote>
<p>在刚才的 sin 函数图形中追加 cos 函数的图形，并尝试使用 pyplot 的添加标题和 x 轴标签名等其他功能</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

plt<span class="token punctuation">.</span>switch_backend<span class="token punctuation">(</span><span class="token string">'TkAgg'</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 绘制sin函数曲线</span>
<span class="token comment" spellcheck="true"># 生成数据</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 以0.1为步长（单位），生成0到6的数据</span>
y1 <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
y2 <span class="token operator">=</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 绘制图形</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y1<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'cos'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y2<span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'cos'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 用虚线绘制</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># x轴标签</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'y'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># y轴标签</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'sin &amp; cos'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 标题</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="20230826071253.png"></p>
<h3 id="1-6-3-显示图像"><a href="#1-6-3-显示图像" class="headerlink" title="1.6.3 显示图像"></a>1.6.3 显示图像</h3><blockquote>
<p>pyplot 提供了显示图形的方法 imshow()。此外，还可以使用 matplotlib.image 里的 imread() 读取图像</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">from</span> matplotlib<span class="token punctuation">.</span>image <span class="token keyword">import</span> imread

img <span class="token operator">=</span> imread<span class="token punctuation">(</span><span class="token string">'lena.jpg'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 读入图像（设定合适的路径）</span>
plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="20230826071634.png"></p>
<h1 id="2-感知机"><a href="#2-感知机" class="headerlink" title="2. 感知机"></a>2. 感知机</h1><blockquote>
<p>感知机(perceptron)是由美国学者 Frank Rosenblatt 在 1957 年提出的。它作为神经网络（深度学习）的起源的算法，是学习神经网络和深度学习的重要一环</p>
</blockquote>
<ul>
<li>严格地说，本章所说的感知机应该称为‘人工神经元’或‘朴素感知机’，但是因为很多基本原理是共通的，所以这里简单称为‘感知机’</li>
</ul>
<h2 id="2-1-感知机是什么"><a href="#2-1-感知机是什么" class="headerlink" title="2.1 感知机是什么"></a>2.1 感知机是什么</h2><blockquote>
<p>感知机接收多个输入信号，输出一个信号。感知机的信号会形成流，向前方传递信息。感知机的信号只有‘流’/‘不流’(1/0)两种取值。本书中，0 表示‘不传递信号’，1 表示‘传递信号’</p>
</blockquote>
<blockquote>
<p>下图是一个接收两个输入信号的感知机的例子。x1、x2 是输入信号，y 是输出信号，w1、w2 是权重。圆形是‘神经元’或‘节点’。输入信号被送往神经元时，会被分别乘以固定的权重(w1x1,w2x2)。神经元会计算传送过来的信号的总和，只有这个总和超过了某个界限值时，才会输出 1。这也称为‘神经元被激活’。这个界限值称为阈值，用符号 θ 表示</p>
</blockquote>
<p><img src="20230826082349.png"><br><img src="20230826082729.png"></p>
<blockquote>
<p>感知机的多个输入信号都有各自固有的权重，发挥着控制各个信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高</p>
</blockquote>
<ul>
<li>权重相当于电流里所说的电阻。电阻是决定电流流动难度的参数，电阻越低，通过的电流就越大。而感知机的权重则是值越大，通过的信号就越大。在控制信号流动难度（或者流动容易度）这一点上的作用是一样的。</li>
</ul>
<h2 id="2-2-简单逻辑电路"><a href="#2-2-简单逻辑电路" class="headerlink" title="2.2 简单逻辑电路"></a>2.2 简单逻辑电路</h2><h3 id="2-2-1-与门（AND-gate）"><a href="#2-2-1-与门（AND-gate）" class="headerlink" title="2.2.1 与门（AND gate）"></a>2.2.1 与门（AND gate）</h3><blockquote>
<p>与门是有两个输入和一个输出的门电路。与门仅在两个输入均为 1 时输出 1，其他时候输出 0</p>
</blockquote>
<p><img src="20230826083311.png" alt="真值表"></p>
<blockquote>
<p>如果用感知机来表示，怎么确定 w1、w2、θ 的值才能满足该真值表？</p>
</blockquote>
<blockquote>
<p>实际上，满足该条件的参数的选择方法有无数个。比如(w1,w2,θ)=(0.5,0.5,0.7) or (w1,w2,θ)=(0.5,0.5,0.8) or (w1,w2,θ)=(1.0,1.0,1.0)时。设定这样的参数后，仅当 x1 和 x2 同时为 1 时，信号的加权总和才会超过给定的阈值 θ。</p>
</blockquote>
<h3 id="2-2-2-与非门-NAND-gate-和或门"><a href="#2-2-2-与非门-NAND-gate-和或门" class="headerlink" title="2.2.2 与非门(NAND gate)和或门"></a>2.2.2 与非门(NAND gate)和或门</h3><blockquote>
<p>与非门就是颠倒了与门的输出。仅当 x1 和 x2 同时为 1 时输出 0，其他时候则输出 1</p>
</blockquote>
<p><img src="20230826084301.png"></p>
<blockquote>
<p>感知机表示与非门，只需要将实现与门的参数值的符号取反就可以实现</p>
</blockquote>
<blockquote>
<p>与门是‘只要有一个输入信号是 1，输出就为 1’的逻辑电路</p>
</blockquote>
<p><img src="20230826084438.png"></p>
<blockquote>
<p>与门、与非门、或门的感知机构造是一样的。它们只有参数的值（权重和阈值）不同。也就是说，相同构造的感知机只要调整参数的值，就可以变成不同的逻辑电路</p>
</blockquote>
<ul>
<li>这里我们人为决定感知机参数，看着真值表这种‘训练数据’，人工考虑了参数的值。而机器学习的课题就是将这个决定参数值的工作交给计算机自动进行。‘学习’是确定合适参数的过程，人要做的就是思考感知机的构造（模型），并将训练数据交给计算机。</li>
</ul>
<h2 id="2-3-感知机的实现"><a href="#2-3-感知机的实现" class="headerlink" title="2.3 感知机的实现"></a>2.3 感知机的实现</h2><h3 id="2-3-1-简单的实现"><a href="#2-3-1-简单的实现" class="headerlink" title="2.3.1 简单的实现"></a>2.3.1 简单的实现</h3><blockquote>
<p>先定义一个接收参数 x1 和 x2 的 AND 函数</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">AND</span><span class="token punctuation">(</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    w1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> theta <span class="token operator">=</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.7</span>
    tmp <span class="token operator">=</span> x1 <span class="token operator">*</span> w1 <span class="token operator">+</span> x2 <span class="token operator">*</span> w2
    <span class="token keyword">if</span> tmp <span class="token operator">&lt;=</span> theta<span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">0</span>
    <span class="token keyword">elif</span> tmp <span class="token operator">></span> theta<span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">1</span>
</code></pre>
<blockquote>
<p>在函数内初始化参数 w1、w2、theta，但输入的加权总和超过阈值时返回 1，否则返回 0</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>AND<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>AND<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>AND<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>AND<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1</span>
</code></pre>
<h3 id="2-3-2-导入权重和偏置"><a href="#2-3-2-导入权重和偏置" class="headerlink" title="2.3.2 导入权重和偏置"></a>2.3.2 导入权重和偏置</h3><blockquote>
<p>将之前的式子的阈值换成偏置，即 θ 换成-b</p>
</blockquote>
<p><img src="20230826094137.png"></p>
<blockquote>
<p>改变后，表达的内容依然完全相同。b 称为偏置，w 称为权重，感知机计算输入信号和权重的乘积，然后加上偏置，如果这个值大于 0 则输出 1，否则输出 0。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 输入</span>
w <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 权重</span>
b <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">0.7</span>  <span class="token comment" spellcheck="true"># 偏置</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>w <span class="token operator">*</span> x<span class="token punctuation">)</span>

np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>w <span class="token operator">*</span> x<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>w <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># -0.19999999999999996 大约为-0.2（由浮点小数造成的运算误差）</span>
</code></pre>
<blockquote>
<p>这里 numpy 数组乘法运算，当两个数组的元素个数相同时，各个元素分别相乘，之后 sum(w*x)计算相乘后的各个元素的总和。最后加上偏置，完成计算</p>
</blockquote>
<h3 id="2-3-3-使用权重和偏置的实现"><a href="#2-3-3-使用权重和偏置的实现" class="headerlink" title="2.3.3 使用权重和偏置的实现"></a>2.3.3 使用权重和偏置的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">AND</span><span class="token punctuation">(</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span>
    w <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">0.7</span>
    tmp <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>w <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b
    <span class="token keyword">if</span> tmp <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">0</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">1</span>
</code></pre>
<blockquote>
<p>权重 w1、w2 是控制输入信号的重要性的参数，而偏置是调整神经元被激活的容易程度（输出信号为 1 的程度）的参数。</p>
</blockquote>
<ul>
<li>偏置这个术语，有‘穿木屐’的效果，即在没有任何输入时（输入为 0 时），给输出穿上多高的木屐（加上多大的值）的意思。</li>
</ul>
<blockquote>
<p>继续实现与非门和或门</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">NAND</span><span class="token punctuation">(</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span>
    w <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> <span class="token number">0.7</span>
    tmp <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>w <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b
    <span class="token keyword">if</span> tmp <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">0</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">1</span>


<span class="token keyword">def</span> <span class="token function">OR</span><span class="token punctuation">(</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span>
    w <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">0.2</span>
    tmp <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>w <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b
    <span class="token keyword">if</span> tmp <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">0</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">1</span>
</code></pre>
<h2 id="2-4-感知机的局限性"><a href="#2-4-感知机的局限性" class="headerlink" title="2.4 感知机的局限性"></a>2.4 感知机的局限性</h2><h3 id="2-4-1-异或门"><a href="#2-4-1-异或门" class="headerlink" title="2.4.1 异或门"></a>2.4.1 异或门</h3><blockquote>
<p>异或门也称为逻辑异或电路。仅当 x1 或 x2 中的一方为 1 时，才会输出 1（‘异或’是拒绝其他的意思）。</p>
</blockquote>
<p><img src="20230826102524.png"></p>
<blockquote>
<p>前面介绍的感知机无法实现这个异或门。</p>
</blockquote>
<blockquote>
<p>我们将或门的动作形象化。或门的情况下，当权重参数(b,w1,w2)=(-0.5,1.0,1.0)时，可以用下面的式子表示</p>
</blockquote>
<p><img src="20230826102720.png"></p>
<blockquote>
<p>该式表示感知机会生成由直线-0.5 + x1 + x2 = 0 分割开的两个空间。其中一个空间输出 1，另一个空间输出 0</p>
</blockquote>
<p><img src="20230826102852.png"></p>
<blockquote>
<p>但是如何用一条直线分割出异或的 0、1 输出空间？事实上，用一条直线无法将 0、1 分开</p>
</blockquote>
<p><img src="20230826103638.png"></p>
<h3 id="2-4-2-线性和非线性"><a href="#2-4-2-线性和非线性" class="headerlink" title="2.4.2 线性和非线性"></a>2.4.2 线性和非线性</h3><blockquote>
<p>感知机的局限性就在于它只能表示由一条直线分割的空间。如果用曲线分割，就可以实现。由曲线分割而成的空间称为非线性空间，由直线分割而成的空间称为线性空间。线性和非线性这两个术语在机器学习领域很常见。</p>
</blockquote>
<p><img src="20230826103910.png"></p>
<h2 id="2-5-多层感知机"><a href="#2-5-多层感知机" class="headerlink" title="2.5 多层感知机"></a>2.5 多层感知机</h2><blockquote>
<p>感知机的绝妙之处在于它可以‘叠加层’（通过叠加层来表示异或门或门是本节的要点）。我们暂不考虑叠加层具体是指什么，先从其他视角来思考一下异或门的问题。</p>
</blockquote>
<h3 id="2-5-1-已有门电路的组合"><a href="#2-5-1-已有门电路的组合" class="headerlink" title="2.5.1 已有门电路的组合"></a>2.5.1 已有门电路的组合</h3><blockquote>
<p>异或门制作方法有很多，其中之一就是与门、或门、与非门的组合。这几个门我们用下面的符号表示，图 2-9 中与非门前端的〇表示反转输出的意思</p>
</blockquote>
<p><img src="20230826104530.png"><br><img src="20230826104549.png"></p>
<p><img src="20230826104752.png"><br><img src="20230826104842.png"></p>
<h3 id="2-5-2-异或门的实现"><a href="#2-5-2-异或门的实现" class="headerlink" title="2.5.2 异或门的实现"></a>2.5.2 异或门的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">XOR</span><span class="token punctuation">(</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    s1 <span class="token operator">=</span> NAND<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span>
    s2 <span class="token operator">=</span> OR<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span>
    y <span class="token operator">=</span> AND<span class="token punctuation">(</span>s1<span class="token punctuation">,</span> s2<span class="token punctuation">)</span>
    <span class="token keyword">return</span> y

<span class="token keyword">print</span><span class="token punctuation">(</span>XOR<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>XOR<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>XOR<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>XOR<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0</span>
</code></pre>
<blockquote>
<p>异或门是一种多层结构的神经网络。这里，将最左边的一列称为第 0 层，中间的一列称为第一层，最右边的一列称为第 2 层。</p>
</blockquote>
<blockquote>
<p>实际上，与门、或门是单层感知机，而异或门是 2 层感知机。叠加了多层的感知机也称为多层感知机(multi-layered perceptron)</p>
</blockquote>
<p><img src="20230826151026.png"></p>
<ul>
<li>上图由 3 层构成，但是因为拥有权重的实际上只有两层（第 0 层到第 1 层，第 1 层和第 2 层之间），所以称为‘2 层感知机’，也有的文献认为这是由 3 层构成，所以称为‘3 层感知机’</li>
</ul>
<blockquote>
<p>在这种多层感知机中，第一层输出变为第二层输入，数据在之间不断传送。通过叠加层，感知机能进行更加灵活的表示。</p>
</blockquote>
<h2 id="2-6-从与非门到计算机"><a href="#2-6-从与非门到计算机" class="headerlink" title="2.6 从与非门到计算机"></a>2.6 从与非门到计算机</h2><blockquote>
<p>多层感知机可以实现比之前见到的电路更复杂的电路。比如加法运算的加法器、二进制转十进制的编码器、满足某些条件就输出 1 的电路（用于等价检验的电路）……甚至可以表示计算机</p>
</blockquote>
<blockquote>
<p>计算机是处理信息的机器。输入信息，会按照某个既定规则进行计算，然后输出，这和感知机是一样的</p>
</blockquote>
<blockquote>
<p>只需要通过与非门的组合，就能再现计算机</p>
</blockquote>
<ul>
<li>拓展阅读：《计算机系统要素：从零开始构建现代计算机》</li>
</ul>
<blockquote>
<p>多少层（层级多深）的感知机可以表示计算机？理论上可以说 2 层感知机就可以构建计算机。有研究证明，2 层感知机（严格地说是激活函数使用了非线性的 sigmoid 函数的感知机）可以表示任意函数。但是使用 2 层感知机的构造，通过设定合适的权重来构建计算机是非常累人的事。</p>
</blockquote>
<blockquote>
<p>实际上，用与非门等低层元件构建计算机的情况下，分阶段地制作所需的零件（模块）会比较自然，即先实现与门和或门，然后实现半加器和全加器，接着实现算数逻辑单元（ALU），然后实现 CPU。使用这种叠加了多层的构造来实现是比较自然的流程。</p>
</blockquote>
<h2 id="2-7-小结"><a href="#2-7-小结" class="headerlink" title="2.7 小结"></a>2.7 小结</h2><ul>
<li>感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值</li>
<li>感知机将权重和偏置设定为参数。</li>
<li>使用感知机可以表示与门和或门等逻辑电路</li>
<li>异或门无法通过单层感知机来表示。</li>
<li>使用 2 层感知机可以表示异或门</li>
<li>单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。</li>
<li>多层感知机（在理论上）可以表示计算机。</li>
</ul>
<h1 id="3-神经网络"><a href="#3-神经网络" class="headerlink" title="3. 神经网络"></a>3. 神经网络</h1><blockquote>
<p>神经网络的出现就是为了解决设定权重的工作，即机器自动从数据中学习，确定合适的、能符合预期的输入与输出的权重。</p>
</blockquote>
<h2 id="3-1-从感知机到神经网络"><a href="#3-1-从感知机到神经网络" class="headerlink" title="3.1 从感知机到神经网络"></a>3.1 从感知机到神经网络</h2><blockquote>
<p>神经网络和感知机有很多共同点，这里主要介绍差异</p>
</blockquote>
<h3 id="3-1-1-神经网络例子"><a href="#3-1-1-神经网络例子" class="headerlink" title="3.1.1 神经网络例子"></a>3.1.1 神经网络例子</h3><blockquote>
<p>下图 3-1 表示神经网络，我们将最左边的一列称为<em>输入层</em>，最右边的一列称为<em>输出层</em>，中间的一列称为<em>中间层</em>。中间层有时也称为<em>隐藏层</em>。‘隐藏’的意思是，隐藏层的神经元（和输入层、输出层不同）肉眼看不见。另外，本书的层号从零开始计算，为了方便用 python 实现神经网络</p>
</blockquote>
<p><img src="20230826154023.png"></p>
<ul>
<li>图 3-1 的网络一共由 3 层神经元组成，当实际上只有 2 层神经元有权重，因此本书将其称为‘2 层网络’。也有的书会把它称为 3 层网络。</li>
</ul>
<blockquote>
<p>那么，神经网络中的信号是如何传递的呢？</p>
</blockquote>
<h3 id="3-1-2-复习感知机"><a href="#3-1-2-复习感知机" class="headerlink" title="3.1.2 复习感知机"></a>3.1.2 复习感知机</h3><blockquote>
<p>思考下图中的网络结构</p>
</blockquote>
<p><img src="20230826163512.png"><br><img src="20230826163534.png"></p>
<blockquote>
<p>b 是被称为偏置的参数，用于控制神经元被激活的容易程度；而 w1 和 w2 是表示各个信号的权重的参数，用于控制各个信号的重要性</p>
</blockquote>
<blockquote>
<p>在图 3-2 中没有把 b 画出来，如果要明确表示 b，可以像图 3-3 那样。图 3-3 添加了权重为 b 的输入信号 1。这个感知机将 x1、x2、1 三个信号作为神经元的输入，将其和各自的权重相乘后，传送至下一个神经元。在下一个神经元中，计算这些加权信号的总和。如果这个总和超过 0，则输出 1，否则输出 0。</p>
</blockquote>
<blockquote>
<p>为了简化式子（3.1），我们引入一个新函数 h(x)来表示这种分情况的动作（超过 0 则输出 1，否则输出 0）。</p>
</blockquote>
<p><img src="20230826164325.png"><br><img src="farfwe.png"></p>
<blockquote>
<p>在式子(3.2)中，输入信号的总和会被函数 h(x)转换，转换后的值就是输出 y。</p>
</blockquote>
<h3 id="3-1-3-激活函数登场"><a href="#3-1-3-激活函数登场" class="headerlink" title="3.1.3 激活函数登场"></a>3.1.3 激活函数登场</h3><blockquote>
<p>h(x)函数会将输入信号的总和转换为输出信号，这种函数一般称为<em>激活函数</em>(activation function)。它的作用在于决定如何来激活输入信号的总和。</p>
</blockquote>
<blockquote>
<p>改写式子（3.2），将其分为两个阶段处理，先计算输入信号的加权总和，然后用激活函数转换这一总和。</p>
</blockquote>
<p><img src="20230826165322.png"></p>
<blockquote>
<p>首先式子（3.4）计算加权输入信号和偏置的总和，记为 a，然后式子（3.5）用 h()函数将 a 转换为输出 y</p>
</blockquote>
<p><img src="20230826165522.png"><br><img src="20230826165629.png"></p>
<ul>
<li>本书在使用‘感知机’一词时，没有严格统一它所指的算法。一般而言，‘朴素感知机’是指单层网络，指的是激活函数使用了阶跃函数的模型。‘多层感知机’是指神经网络，即使用 sigmoid 函数等平滑的激活函数的多层网络。</li>
</ul>
<h2 id="3-2-激活函数"><a href="#3-2-激活函数" class="headerlink" title="3.2 激活函数"></a>3.2 激活函数</h2><blockquote>
<p>式子（3.3）表示的激活函数以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为‘阶跃函数’。感知机中使用了阶跃函数作为激活函数。如果使用其他的激活函数，就可以进入神经网络的世界了。</p>
</blockquote>
<h3 id="3-2-1-sigmoid-函数"><a href="#3-2-1-sigmoid-函数" class="headerlink" title="3.2.1 sigmoid 函数"></a>3.2.1 sigmoid 函数</h3><blockquote>
<p>神经网络中最常使用的一个激活函数就是 sigmoid 函数</p>
</blockquote>
<p><img src="20230826190132.png" alt="3.6"></p>
<blockquote>
<p>exp(-x)是 e^(-x)的意思。e 是纳皮尔常数 2.7182···。函数，就是给定某个输入后，会返回某个输出的转换器。</p>
</blockquote>
<blockquote>
<p>神经网络中用 sigmoid 函数作为激活函数，进行信号的转换，转换后的信号被传送给下一个神经元。感知机和神经网络的主要区别就在于这个激活函数。其他方面，比如多层连接的构造、信号的传递方法等，基本和感知机一致。</p>
</blockquote>
<h3 id="3-2-2-阶跃函数的实现"><a href="#3-2-2-阶跃函数的实现" class="headerlink" title="3.2.2 阶跃函数的实现"></a>3.2.2 阶跃函数的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step_function</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> x <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">1</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">0</span>
</code></pre>
<blockquote>
<p>改为支持 numpy 数组的实现</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step_function</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    y <span class="token operator">=</span> x <span class="token operator">></span> <span class="token number">0</span>
    <span class="token keyword">return</span> y<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>上面使用了 numpy 的技巧</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> x <span class="token operator">></span> <span class="token number">0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [False  True  True]</span>
</code></pre>
<blockquote>
<p>在条件运算后，符合条件的变为 true，不符合的变为 false，生成一个布尔型数组。但是阶跃函数需要输出 int 类型，所以需要转换</p>
</blockquote>
<pre class=" language-python"><code class="language-python">y <span class="token operator">=</span> y<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [0 1 1]</span>
</code></pre>
<h3 id="3-2-3-阶跃函数的图形"><a href="#3-2-3-阶跃函数的图形" class="headerlink" title="3.2.3 阶跃函数的图形"></a>3.2.3 阶跃函数的图形</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pylab <span class="token keyword">as</span> plt

plt<span class="token punctuation">.</span>switch_backend<span class="token punctuation">(</span><span class="token string">'TkAgg'</span><span class="token punctuation">)</span>


<span class="token comment" spellcheck="true"># 阶跃函数</span>
<span class="token keyword">def</span> <span class="token function">step_function</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int<span class="token punctuation">)</span>


x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">5.0</span><span class="token punctuation">,</span> <span class="token number">5.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> step_function<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1.1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 指定y轴的范围</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="20230827142237.png"></p>
<blockquote>
<p>阶跃函数以 0 为界，输出从 0 开始切换为 1（或者从 1 切换为 0），值呈阶梯式变化，所以称为阶跃函数</p>
</blockquote>
<h3 id="3-2-4-sigmoid-函数的实现"><a href="#3-2-4-sigmoid-函数的实现" class="headerlink" title="3.2.4 sigmoid 函数的实现"></a>3.2.4 sigmoid 函数的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># exp(-x) -> e^(-x)</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>


x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [0.26894142 0.73105858 0.88079708]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>该函数可以支持 np 数组，因为 np 有广播机制，可以支持标量和数组的运算，会将标量的计算运用到每个数组元素</p>
</blockquote>
<blockquote>
<p>画图</p>
</blockquote>
<pre class=" language-python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">5.0</span><span class="token punctuation">,</span> <span class="token number">5.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1.1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 指定y轴的范围</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="20230827143147.png"></p>
<h3 id="3-2-5-sigmoid-函数和阶跃函数的比较"><a href="#3-2-5-sigmoid-函数和阶跃函数的比较" class="headerlink" title="3.2.5 sigmoid 函数和阶跃函数的比较"></a>3.2.5 sigmoid 函数和阶跃函数的比较</h3><blockquote>
<p>首先，平滑性不同：sigmoid 函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以 0 为界，输出发生急剧性的变化。sigmoid 函数的平滑性对神经网络的学习具有重要意义。</p>
</blockquote>
<p><img src="20230827185640.png"></p>
<blockquote>
<p>另一个不同点是，阶跃函数只能返回 0 或 1，而 sigmoid 可以返回 0.731…、0.880…等实数。也就是说，感知机中神经元之间流动的是 0 或 1 的二元信号，而神经网络中流动的是连续的实数值信号。</p>
</blockquote>
<blockquote>
<p>虽然它们在平滑性上有差异，但是从宏观视角看，有着相似的形状。它们的结构都是‘输入小时，输出接近 0（为 0）；随着输入增大，输出向 1 靠近（变成 1）’。即，当输入信号为重要信息时，阶跃函数和 sigmoid 函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值。还有一个共同点，不管输入信号多小或多大，输出信号都在 0 到 1 之间。</p>
</blockquote>
<h3 id="3-2-6-非线性函数"><a href="#3-2-6-非线性函数" class="headerlink" title="3.2.6 非线性函数"></a>3.2.6 非线性函数</h3><blockquote>
<p>还有一个共同点，sigmoid 函数是一条曲线，阶跃函数是一条像阶梯一样的折线。两者都属于<em>非线性函数</em></p>
</blockquote>
<ul>
<li>函数是输入某个值后会返回一个值的转换器。而这个转换器输入某个值后，输出值是输入值的常数倍的函数称为线性函数（h(x)=cx）。因此，线性函数是一条笔直的直线。</li>
</ul>
<blockquote>
<p>神经网络的激活函数必须使用非线性函数。因为使用线性函数的话，加深神经网络的层数就没意义了。</p>
</blockquote>
<blockquote>
<p>线性函数的问题在于，无论如何加深层数，总是存在与之等效的‘无隐藏层的神经网络’。比如线性函数 h(x)=cx 作为激活函数，把 y(x)=h(h(h(x)))的运算对应 3 层神经网络。这个运算会进行 y(x)=c * c * c * x 的乘法运算，但是同样的处理可以由 y(x)=ax (a=c^3) 这一没有隐藏层的神经网络来表示。也就是说，线性函数作为激活函数，无法发挥多层网络带来的优势</p>
</blockquote>
<h3 id="3-2-7-ReLU-函数"><a href="#3-2-7-ReLU-函数" class="headerlink" title="3.2.7 ReLU 函数"></a>3.2.7 ReLU 函数</h3><blockquote>
<p>在神经网络的发展历史上，很早就开始使用 sigmoid 函数了，最近则主要使用 ReLU(Rectified Linear Unit)函数</p>
</blockquote>
<blockquote>
<p>ReLU 函数在输入大于 0 时，直接输出该值；在输入小于等于 0 时，输出 0</p>
</blockquote>
<p><img src="20230827194658.png"></p>
<blockquote>
<p>ReLU 的代码实现很简单</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 大于0输出x,小于0输出0</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>
</code></pre>
<p><img src="20230827194847.png"></p>
<blockquote>
<p>这里使用了 numpy 的 maximum 函数，它会从输入的数值中选择较大的那个值进行输出</p>
</blockquote>
<h2 id="3-3-多维数组的运算"><a href="#3-3-多维数组的运算" class="headerlink" title="3.3 多维数组的运算"></a>3.3 多维数组的运算</h2><blockquote>
<p>掌握多维数组的运算，就可以高效地实现神经网络。</p>
</blockquote>
<h3 id="3-3-1-多维数组"><a href="#3-3-1-多维数组" class="headerlink" title="3.3.1 多维数组"></a>3.3.1 多维数组</h3><blockquote>
<p>多维数组就是‘数字的集合’，数字排成一列的集合、排成长方形的集合、排成三维状或（更一般化的）N 维状的集合……</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [1 2 3 4]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>ndim<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 4</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>数组的维数可以通过 np.ndim()函数获得。此外，数组的形状可以通过实例变量 shape 获得。A.shape 的结果是个元组</p>
</blockquote>
<blockquote>
<p>下面生成二维数组</p>
</blockquote>
<pre class=" language-python"><code class="language-python">B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [[1 2]</span>
<span class="token comment" spellcheck="true">#  [3 4]</span>
<span class="token comment" spellcheck="true">#  [5 6]]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 2</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>ndim<span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (3, 2)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>3x2 表示第一个维度有 3 个元素，第二个维度有 2 个元素。第一个维度是第 0 维，第二个维度是第 1 维（索引从 0 开始）。二维数组也称为矩阵（matrix）。数组的横向排列称为行（row），纵向排列称为列（column）。</p>
</blockquote>
<h3 id="3-3-2-矩阵乘法"><a href="#3-3-2-矩阵乘法" class="headerlink" title="3.3.2 矩阵乘法"></a>3.3.2 矩阵乘法</h3><p><img src="20230827200414.png"></p>
<blockquote>
<p>矩阵的乘积是通过左边矩阵的行（横向）和右边矩阵的列（纵向）以对应元素的方式相乘后再求和而得到的。并且，运算的结果保存为新的多维数组的元素。比如<em>A</em>的第一行和<em>B</em>的第一列的乘积结果是新数组的第一行第一列的元素。</p>
</blockquote>
<pre class=" language-python"><code class="language-python">A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (2, 2)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (2, 2)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [[19 22]</span>
<span class="token comment" spellcheck="true">#  [43 50]]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>乘积也叫点积，可以用 np.dot()计算。和一般的运算（+或*等）不同，矩阵的乘积运算中，操作数（A、B）的顺序不同，结果也会不同。</p>
</blockquote>
<pre class=" language-python"><code class="language-python">A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (2, 3)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (3, 2)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [[22 28]</span>
<span class="token comment" spellcheck="true">#  [49 64]]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>注意矩阵的形状，<em>A</em>的第一维的元素个数（列数）必须和<em>B</em>的第 0 维的元素个数（行数）相等，才能进行乘法计算</p>
</blockquote>
<pre class=" language-python"><code class="language-python">C <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (2, 2)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>C<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (2, 3)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">#   File "&lt;__array_function__ internals>", line 6, in dot</span>
<span class="token comment" spellcheck="true"># ValueError: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>A<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="20230827202356.png"></p>
<blockquote>
<p>运算结果的形状是由<em>A</em>的行数和<em>B</em>的列数构成的。</p>
</blockquote>
<pre class=" language-python"><code class="language-python">A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (3, 2)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (2,)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [23 53 83]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="20230827202616.png"></p>
<h3 id="3-3-3-神经网络的内积"><a href="#3-3-3-神经网络的内积" class="headerlink" title="3.3.3 神经网络的内积"></a>3.3.3 神经网络的内积</h3><blockquote>
<p>我们使用 numpy 矩阵来实现神经网络，这里省略了偏置和激活函数</p>
</blockquote>
<p><img src="20230827202650.png"></p>
<pre class=" language-python"><code class="language-python">X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (2,)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
W <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [[1 3 5]</span>
<span class="token comment" spellcheck="true">#  [2 4 6]]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>W<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (2, 3)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>W<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (1,2) * (2,3) -> (1,3)</span>
Y <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X<span class="token punctuation">,</span> W<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [ 5 11 17]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>Y<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>使用 np.dot 可以一次运算出结果，这种技巧很重要。</p>
</blockquote>
<h2 id="3-4-三层神经网络的实现"><a href="#3-4-三层神经网络的实现" class="headerlink" title="3.4 三层神经网络的实现"></a>3.4 三层神经网络的实现</h2><blockquote>
<p>我们实现从输入到输出的（前向）处理。</p>
</blockquote>
<p><img src="20230827203745.png"></p>
<h3 id="3-4-1-符号确认"><a href="#3-4-1-符号确认" class="headerlink" title="3.4.1 符号确认"></a>3.4.1 符号确认</h3><blockquote>
<p>我们引入 w12^(1)和 a1^(1)等符号。</p>
</blockquote>
<blockquote>
<p>在下图中，权重和隐藏层的神经元的右上角有一个”(1)”，它表示权重和神经元的层号（即第一层的权重、第一层的神经元）。此外，权重的右下角有两个数字，它们是后一层的神经元和前一层的神经元的索引号。比如 w12^(1)表示前一层的第 2 个神经元 x2 到后一层的第 1 个神经元 a1^(1)的权重。权重右下角按照“后一层的索引号、前一层的索引号”的顺序排列。</p>
</blockquote>
<p><img src="20230827204355.png"></p>
<h3 id="3-4-2-各层间信号传递的实现"><a href="#3-4-2-各层间信号传递的实现" class="headerlink" title="3.4.2 各层间信号传递的实现"></a>3.4.2 各层间信号传递的实现</h3><blockquote>
<p>看一下从输入层到第 1 层的第 1 个神经元的信号传递过程。</p>
</blockquote>
<p><img src="20230827204956.png"></p>
<blockquote>
<p>图中新增了表示偏置的神经元”1”。它的右下角的索引号只有一个，因为前一层的偏置神经元（神经元‘1’）只有一个。</p>
</blockquote>
<ul>
<li>任何前一层的偏置神经元‘1’都只有一个。偏置权重的数量取决于后一层的神经元的数量（不包括后一层的偏置神经元‘1’）————译者注</li>
</ul>
<blockquote>
<p>用数学式表示 a1^(1)。通过加权信号和偏置的和按如下方式进行计算</p>
</blockquote>
<p><img src="20230827205329.png"></p>
<blockquote>
<p>用矩阵的乘法运算，可以间第一层的加权和表示成下面的式</p>
</blockquote>
<p><img src="20230827205604.png"></p>
<pre class=" language-python"><code class="language-python">X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
W1 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B1 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>W1<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (2, 3)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (2,)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>B1<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (3,)</span>

A1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X<span class="token punctuation">,</span> W1<span class="token punctuation">)</span> <span class="token operator">+</span> B1
</code></pre>
<blockquote>
<p>接下来，隐藏层的加权和（加权信号和偏置的总和）用 a 表示，被激活函数转换后的信号用 z 表示。此外，图中 h()表示激活函数，这里使用的是 sigmoid 行数。</p>
</blockquote>
<pre class=" language-python"><code class="language-python">Z1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>A1<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A1<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [0.3 0.7 1.1]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>Z1<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [0.57444252 0.66818777 0.75026011]</span>
</code></pre>
<p><img src="20230827211607.png"></p>
<blockquote>
<p>下面来实现第 1 层到第 2 层的信号传递</p>
</blockquote>
<pre class=" language-python"><code class="language-python">W2 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B2 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>Z1<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (3,)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>W2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (3, 2)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>B2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (2,)</span>

A2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>Z1<span class="token punctuation">,</span> W2<span class="token punctuation">)</span> <span class="token operator">+</span> B2
Z2 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>A2<span class="token punctuation">)</span>
</code></pre>
<p><img src="20230827211941.png"></p>
<blockquote>
<p>最好是第 2 层到输出层的信号传递。输出层的实现也和之前的实现基本相同。不过，最后的激活函数和之前的隐藏层有所不同</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">identity_function</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x


W3 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B3 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

A3 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>Z2<span class="token punctuation">,</span> W3<span class="token punctuation">)</span> <span class="token operator">+</span> B3
Y <span class="token operator">=</span> identity_function<span class="token punctuation">(</span>A3<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 或 Y = A3</span>
</code></pre>
<blockquote>
<p>这里定义了 identity_function（也叫恒等函数），会将输入按原样输出，其实没必要定义这个，这里是为了和之前的流程保持统一。输出层的激活函数用 σ()表示（σ 读作 sigma），不同于隐藏层的激活函数 h()</p>
</blockquote>
<p><img src="20230827212756.png"></p>
<ul>
<li>输出层所用的激活函数要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid 函数，多元分类问题可以使用 softmax 函数。</li>
</ul>
<h3 id="3-4-3-代码实现小结"><a href="#3-4-3-代码实现小结" class="headerlink" title="3.4.3 代码实现小结"></a>3.4.3 代码实现小结</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">init_network</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    network <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    network<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'W3'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'b3'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> network


<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>network<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    W1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> W3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W3'</span><span class="token punctuation">]</span>
    b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b3'</span><span class="token punctuation">]</span>

    a1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W1<span class="token punctuation">)</span> <span class="token operator">+</span> b1
    z1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a1<span class="token punctuation">)</span>
    a2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z1<span class="token punctuation">,</span> W2<span class="token punctuation">)</span> <span class="token operator">+</span> b2
    z2 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a2<span class="token punctuation">)</span>
    a3 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z2<span class="token punctuation">,</span> W3<span class="token punctuation">)</span> <span class="token operator">+</span> b3
    y <span class="token operator">=</span> identity_function<span class="token punctuation">(</span>a3<span class="token punctuation">)</span>

    <span class="token keyword">return</span> y


network <span class="token operator">=</span> init_network<span class="token punctuation">(</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> forward<span class="token punctuation">(</span>network<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [0.31682708 0.69627909]</span>
</code></pre>
<h2 id="3-5-输出层的设计"><a href="#3-5-输出层的设计" class="headerlink" title="3.5 输出层的设计"></a>3.5 输出层的设计</h2><blockquote>
<p>神经网络可以用在分类问题和回归问题上，根据情况改变输出层的激活函数。一般地，回归问题用恒等函数，分类问题用 softmax 函数。</p>
</blockquote>
<ul>
<li>机器学习的问题大致可以分为分类问题和回归问题。分类问题是数据属于哪一个类别的问题。回归问题是根据某个输入预测一个（连续的）数值的问题</li>
</ul>
<h3 id="3-5-1-恒等函数和-softmax-函数"><a href="#3-5-1-恒等函数和-softmax-函数" class="headerlink" title="3.5.1 恒等函数和 softmax 函数"></a>3.5.1 恒等函数和 softmax 函数</h3><blockquote>
<p>恒等函数将输入按原样输出，对于输入的信息，不加任何改动地直接输出。</p>
</blockquote>
<p><img src="20230828142714.png"></p>
<blockquote>
<p>exp(x)表示 e^x 的指数函数（e 是纳皮尔常数 2.7182···），假设输出层共有 n 个神经元，计算第 k 个神经元的输出 yk。softmax 函数的分字是输入信号 ak 的指数函数，分母是所有输入信号的指数函数的和。</p>
</blockquote>
<blockquote>
<p>用图表示 softmax 函数的话，可以看出，softmax 函数的输出通过箭头与所有输入信号相连。输出层的各个神经元都受到所有输入信号的影响。</p>
</blockquote>
<p><img src="20230828143109.png"></p>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># softmax</span>
a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">2.9</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 指数函数</span>
<span class="token comment" spellcheck="true"># [ 1.34985881 18.17414537 54.59815003]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>

sum_exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 指数函数的和</span>
<span class="token comment" spellcheck="true"># 74.1221542101633</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sum_exp_a<span class="token punctuation">)</span>

y <span class="token operator">=</span> exp_a <span class="token operator">/</span> sum_exp_a
<span class="token comment" spellcheck="true"># [0.01821127 0.24519181 0.73659691]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    sum_exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>
    y <span class="token operator">=</span> exp_a <span class="token operator">/</span> sum_exp_a

    <span class="token keyword">return</span> y
</code></pre>
<h3 id="3-5-2-实现-softmax-函数时的注意事项"><a href="#3-5-2-实现-softmax-函数时的注意事项" class="headerlink" title="3.5.2 实现 softmax 函数时的注意事项"></a>3.5.2 实现 softmax 函数时的注意事项</h3><blockquote>
<p>上面的实现虽然可以表示 softmax，但是会导致溢出问题。因为 softmax 涉及指数运算，而指数运算的值通常很大，比如 e^10 的值超过 20000，e^100 后面带 40 多个 0，e^1000 的结果返回一个表示无穷大的 inf。</p>
</blockquote>
<ul>
<li>计算机在处理‘数’时，数值必须在 4~8 字节的有限数据宽度内。这意味着数存在有效位数，可以表示的数值范围是有限的。因此，会出现超大值无法表示的问题（溢出问题）</li>
</ul>
<blockquote>
<p>可以改进 softmax 函数</p>
</blockquote>
<p><img src="20230828143906.png"></p>
<blockquote>
<p>在 softmax 函数的分子分母上都乘 C（任意常数），然后把 C 移动到指数函数 exp 中，记为 logC，最后，把 logC 替换为 C’</p>
</blockquote>
<blockquote>
<p>这里的 C’可以是任何值，但是为了防止溢出，一般会使用输入信号中的最大值。</p>
</blockquote>
<pre class=" language-python"><code class="language-python">a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1010</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">990</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [nan nan nan]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># softmax函数的计算</span>

c <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [  0 -10 -20]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a <span class="token operator">-</span> c<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># [9.99954600e-01 4.53978686e-05 2.06106005e-09]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a <span class="token operator">-</span> c<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a <span class="token operator">-</span> c<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    c <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a <span class="token operator">-</span> c<span class="token punctuation">)</span>
    sum_exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>
    y <span class="token operator">=</span> exp_a <span class="token operator">/</span> sum_exp_a

    <span class="token keyword">return</span> y
</code></pre>
<h3 id="3-5-3-softmax-函数的特征"><a href="#3-5-3-softmax-函数的特征" class="headerlink" title="3.5.3 softmax 函数的特征"></a>3.5.3 softmax 函数的特征</h3><pre class=" language-python"><code class="language-python">a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">2.9</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [0.01821127 0.24519181 0.73659691]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 1.0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>softmax 的输出是 0~1 之间的实数。而且输出值的总和为 1。这是一个重要特性，因为有了这个性质，我们才把 softmax 函数的输出解释为‘概率’</p>
</blockquote>
<blockquote>
<p>需要注意的是，即使用了 softmax 函数，各个元素间的大小关系也不会改变。这是因为指数函数（y=exp(x)）是单调递增函数。</p>
</blockquote>
<blockquote>
<p>一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即使使用 softmax 函数，输出值最大的神经元的位置也不会变。因此，神经网络进行分类时，输出层的 softmax 函数可以省略。</p>
</blockquote>
<ul>
<li>求解机器学习问题的步骤分为‘学习’和‘推理’两个阶段。推理阶段一般会忽略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关</li>
</ul>
<h3 id="3-5-4-输出层的神经元数量"><a href="#3-5-4-输出层的神经元数量" class="headerlink" title="3.5.4 输出层的神经元数量"></a>3.5.4 输出层的神经元数量</h3><blockquote>
<p>输出层的神经元数量需要根据待解决的问题来决定。对于分类问题，输出层的神经元数量一般设定为类别的数量。</p>
</blockquote>
<p><img src="20230828150820.png"></p>
<h2 id="3-6-手写数字识别"><a href="#3-6-手写数字识别" class="headerlink" title="3.6 手写数字识别"></a>3.6 手写数字识别</h2><blockquote>
<p>我们假设学习已经结束，使用学习到的参数，先实现神经网络的‘推理处理’。这个推理处理也称为神经网络的<em>前向传播</em>(forward propagation)</p>
</blockquote>
<ul>
<li>使用神经网络解决问题时，也需要首先使用训练数据（学习数据）进行权重参数的学习；进行推理时，使用刚才学习到的参数，对输入的数据进行分类</li>
</ul>
<h3 id="3-6-1-MNIST-数据集"><a href="#3-6-1-MNIST-数据集" class="headerlink" title="3.6.1 MNIST 数据集"></a>3.6.1 MNIST 数据集</h3><blockquote>
<p>MNIST 数据集是由 0 到 9 的数字图像构成的。训练图像有 6 万多张，测试图像有 1 万多张，这些图像可用于学习和推理。一般使用方法是，先用训练图像进行学习，再用学习到的模型度量能在多大程度上对测试图像进行正确的分类</p>
</blockquote>
<p><img src="20230828201330.png"></p>
<blockquote>
<p>MNIST 的图像数据是 28x28 像素的灰度图像（1 通道），各个通道的取值在 0 到 255 之间。每个图像数据都相应地标有‘7’、‘2’、‘1’等标签。</p>
</blockquote>
<blockquote>
<p>本书提供了脚本 mnist.py 来下载 MNIST 数据集并进行了转化为 numpy 数组等处理</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># coding: utf-8</span>
<span class="token keyword">try</span><span class="token punctuation">:</span>
    <span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request
<span class="token keyword">except</span> ImportError<span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ImportError<span class="token punctuation">(</span><span class="token string">'You should use Python 3.x'</span><span class="token punctuation">)</span>
<span class="token keyword">import</span> os<span class="token punctuation">.</span>path
<span class="token keyword">import</span> gzip
<span class="token keyword">import</span> pickle
<span class="token keyword">import</span> os
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np


url_base <span class="token operator">=</span> <span class="token string">'http://yann.lecun.com/exdb/mnist/'</span>
key_file <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">'train_img'</span><span class="token punctuation">:</span><span class="token string">'train-images-idx3-ubyte.gz'</span><span class="token punctuation">,</span>
    <span class="token string">'train_label'</span><span class="token punctuation">:</span><span class="token string">'train-labels-idx1-ubyte.gz'</span><span class="token punctuation">,</span>
    <span class="token string">'test_img'</span><span class="token punctuation">:</span><span class="token string">'t10k-images-idx3-ubyte.gz'</span><span class="token punctuation">,</span>
    <span class="token string">'test_label'</span><span class="token punctuation">:</span><span class="token string">'t10k-labels-idx1-ubyte.gz'</span>
<span class="token punctuation">}</span>

dataset_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>__file__<span class="token punctuation">)</span><span class="token punctuation">)</span>
save_file <span class="token operator">=</span> dataset_dir <span class="token operator">+</span> <span class="token string">"/mnist.pkl"</span>

train_num <span class="token operator">=</span> <span class="token number">60000</span>
test_num <span class="token operator">=</span> <span class="token number">10000</span>
img_dim <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>
img_size <span class="token operator">=</span> <span class="token number">784</span>


<span class="token keyword">def</span> <span class="token function">_download</span><span class="token punctuation">(</span>file_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
    file_path <span class="token operator">=</span> dataset_dir <span class="token operator">+</span> <span class="token string">"/"</span> <span class="token operator">+</span> file_name

    <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>file_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Downloading "</span> <span class="token operator">+</span> file_name <span class="token operator">+</span> <span class="token string">" ... "</span><span class="token punctuation">)</span>
    urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlretrieve<span class="token punctuation">(</span>url_base <span class="token operator">+</span> file_name<span class="token punctuation">,</span> file_path<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Done"</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">download_mnist</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> v <span class="token keyword">in</span> key_file<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
       _download<span class="token punctuation">(</span>v<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">_load_label</span><span class="token punctuation">(</span>file_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
    file_path <span class="token operator">=</span> dataset_dir <span class="token operator">+</span> <span class="token string">"/"</span> <span class="token operator">+</span> file_name

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Converting "</span> <span class="token operator">+</span> file_name <span class="token operator">+</span> <span class="token string">" to NumPy Array ..."</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> gzip<span class="token punctuation">.</span>open<span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            labels <span class="token operator">=</span> np<span class="token punctuation">.</span>frombuffer<span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>uint8<span class="token punctuation">,</span> offset<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Done"</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> labels

<span class="token keyword">def</span> <span class="token function">_load_img</span><span class="token punctuation">(</span>file_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
    file_path <span class="token operator">=</span> dataset_dir <span class="token operator">+</span> <span class="token string">"/"</span> <span class="token operator">+</span> file_name

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Converting "</span> <span class="token operator">+</span> file_name <span class="token operator">+</span> <span class="token string">" to NumPy Array ..."</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> gzip<span class="token punctuation">.</span>open<span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            data <span class="token operator">=</span> np<span class="token punctuation">.</span>frombuffer<span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>uint8<span class="token punctuation">,</span> offset<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
    data <span class="token operator">=</span> data<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> img_size<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Done"</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> data

<span class="token keyword">def</span> <span class="token function">_convert_numpy</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    dataset <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    dataset<span class="token punctuation">[</span><span class="token string">'train_img'</span><span class="token punctuation">]</span> <span class="token operator">=</span>  _load_img<span class="token punctuation">(</span>key_file<span class="token punctuation">[</span><span class="token string">'train_img'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    dataset<span class="token punctuation">[</span><span class="token string">'train_label'</span><span class="token punctuation">]</span> <span class="token operator">=</span> _load_label<span class="token punctuation">(</span>key_file<span class="token punctuation">[</span><span class="token string">'train_label'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    dataset<span class="token punctuation">[</span><span class="token string">'test_img'</span><span class="token punctuation">]</span> <span class="token operator">=</span> _load_img<span class="token punctuation">(</span>key_file<span class="token punctuation">[</span><span class="token string">'test_img'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    dataset<span class="token punctuation">[</span><span class="token string">'test_label'</span><span class="token punctuation">]</span> <span class="token operator">=</span> _load_label<span class="token punctuation">(</span>key_file<span class="token punctuation">[</span><span class="token string">'test_label'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> dataset

<span class="token keyword">def</span> <span class="token function">init_mnist</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    download_mnist<span class="token punctuation">(</span><span class="token punctuation">)</span>
    dataset <span class="token operator">=</span> _convert_numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Creating pickle file ..."</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> open<span class="token punctuation">(</span>save_file<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> f<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Done!"</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">_change_one_hot_label</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    T <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>size<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> idx<span class="token punctuation">,</span> row <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>
        row<span class="token punctuation">[</span>X<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>

    <span class="token keyword">return</span> T


<span class="token keyword">def</span> <span class="token function">load_mnist</span><span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> flatten<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> one_hot_label<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""读入MNIST数据集

    Parameters
    ----------
    normalize : 将图像的像素值正规化为0.0~1.0
    one_hot_label :
        one_hot_label为True的情况下，标签作为one-hot数组返回
        one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组
    flatten : 是否将图像展开为一维数组

    Returns
    -------
    (训练图像, 训练标签), (测试图像, 测试标签)
    """</span>
    <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>save_file<span class="token punctuation">)</span><span class="token punctuation">:</span>
        init_mnist<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> open<span class="token punctuation">(</span>save_file<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        dataset <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span>

    <span class="token keyword">if</span> normalize<span class="token punctuation">:</span>
        <span class="token keyword">for</span> key <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'train_img'</span><span class="token punctuation">,</span> <span class="token string">'test_img'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            dataset<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> dataset<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
            dataset<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">/=</span> <span class="token number">255.0</span>

    <span class="token keyword">if</span> one_hot_label<span class="token punctuation">:</span>
        dataset<span class="token punctuation">[</span><span class="token string">'train_label'</span><span class="token punctuation">]</span> <span class="token operator">=</span> _change_one_hot_label<span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token string">'train_label'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        dataset<span class="token punctuation">[</span><span class="token string">'test_label'</span><span class="token punctuation">]</span> <span class="token operator">=</span> _change_one_hot_label<span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token string">'test_label'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> <span class="token operator">not</span> flatten<span class="token punctuation">:</span>
         <span class="token keyword">for</span> key <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'train_img'</span><span class="token punctuation">,</span> <span class="token string">'test_img'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            dataset<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> dataset<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> <span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token string">'train_img'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dataset<span class="token punctuation">[</span><span class="token string">'train_label'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token string">'test_img'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dataset<span class="token punctuation">[</span><span class="token string">'test_label'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    init_mnist<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>使用方式</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>pardir<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 为了导入父目录中的文件而进行的设定</span>
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist

<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>flatten<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> normalize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 输出各个数据的形状</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (60000, 784)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (60000,)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (10000, 784)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (10000,)</span>
</code></pre>
<blockquote>
<p>load_mnist 函数有三个参数，比如 load_mnist(normalize=True, flatten=True, one_hot_label=False)中，</p>
</blockquote>
<ul>
<li>第一个参数 normalize 设置是否将输入图像正规化（正则化）为 0.0<del>1.0 的值。如果为 False，则输入图像的像素会保持原来的 0</del>255。</li>
<li>第 2 个参数 flatten 设置是否展开输入图像（变成 1 维数组）。如果为 False，则输入图像为 1x28x28 的三位数组，如果为 True，则输入图像会保存为由 784 个元素构成的一维数组。</li>
<li>第三个参数设置是否将标签保存为 onehot 表示(one-hot representation)。one-hot 表示是仅正确解标签为 1，其余为 0 的数组，如[0,0,1,0,0,0,0,0,0,0]。当 one_hot_label 为 False 时，只是像 7、2 这样简单地保存正确解标签；当为 True 时，则保存为 onehot 表示</li>
</ul>
<blockquote>
<p>python 有 pickle 这个便利的功能。可以将程序运行中的对象保存为文件。如果加载保存过的 pickle 文件，可以立刻复原之前程序运行中的对象。load_mnist 就是利用了这个功能，在第二次读取时快速读取保存在本地的数据集 pkl</p>
</blockquote>
<blockquote>
<p>我们用 PIL(Python Image Library)来显示图像</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>pardir<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 为了导入父目录中的文件而进行的设定</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image


<span class="token keyword">def</span> <span class="token function">img_show</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">:</span>
    pil_img <span class="token operator">=</span> Image<span class="token punctuation">.</span>fromarray<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">)</span>
    pil_img<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>flatten<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> normalize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

img <span class="token operator">=</span> x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
label <span class="token operator">=</span> t_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>label<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 5</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (784,)</span>
img <span class="token operator">=</span> img<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 把图像的形状变成原来的尺寸</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (28, 28)</span>

img_show<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>这里使用了 flatten=True，所有读入的是一维数组，在显示图像时需要（通过 reshape）转为原本的尺寸（28x28），而图像被保存为 numpy 数组，所以需要通过 Image.fromarray()来转换为图像</p>
</blockquote>
<p><img src="20230828203804.png"></p>
<h3 id="3-6-2-神经网络的推理处理"><a href="#3-6-2-神经网络的推理处理" class="headerlink" title="3.6.2 神经网络的推理处理"></a>3.6.2 神经网络的推理处理</h3><blockquote>
<p>接下来实现推理处理。首先，输入层有 784 个神经元（图像大小 28x28=784），输出层有 10 个神经元（0~9，是 10 分类）。此外，这个神经网络有两个隐藏层，第一个隐藏层有 50 个神经元，第二个隐藏层有 100 个神经元。（50 和 100 可以设置为任意值）</p>
</blockquote>
<blockquote>
<p>先定义函数</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> \
        load_mnist<span class="token punctuation">(</span>flatten<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> one_hot_label<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x_test<span class="token punctuation">,</span> t_test


<span class="token keyword">def</span> <span class="token function">init_network</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'sample_weight.pkl'</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        network <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span>

    <span class="token keyword">return</span> network


<span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>network<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    W1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> W3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W3'</span><span class="token punctuation">]</span>
    b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b3'</span><span class="token punctuation">]</span>

    a1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W1<span class="token punctuation">)</span> <span class="token operator">+</span> b1
    z1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a1<span class="token punctuation">)</span>
    a2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z1<span class="token punctuation">,</span> W2<span class="token punctuation">)</span> <span class="token operator">+</span> b2
    z2 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a2<span class="token punctuation">)</span>
    a3 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z2<span class="token punctuation">,</span> W3<span class="token punctuation">)</span> <span class="token operator">+</span> b3
    y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>a3<span class="token punctuation">)</span>

    <span class="token keyword">return</span> y
</code></pre>
<blockquote>
<p>init_network()会读入保存在 pickle 文件 sample_weight.pkl 中的学习到的权重参数。这个文件以字典变量的形式保存了权重和偏置参数。这里假设学习已经完成，所以直接加载 pkl 文件</p>
</blockquote>
<blockquote>
<p>现在我们用这 3 个函数实现神经网络的推理处理。然后，评价它的<em>识别精度</em>(accuracy)，即能在多大程度上正确分类</p>
</blockquote>
<pre class=" language-python"><code class="language-python">x<span class="token punctuation">,</span> t <span class="token operator">=</span> get_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
network <span class="token operator">=</span> init_network<span class="token punctuation">(</span><span class="token punctuation">)</span>

accuracy_cnt <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    y <span class="token operator">=</span> predict<span class="token punctuation">(</span>network<span class="token punctuation">,</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
    p <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 获取概率最高的元素的索引</span>
    <span class="token keyword">if</span> p <span class="token operator">==</span> t<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>
        accuracy_cnt <span class="token operator">+=</span> <span class="token number">1</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Accuracy: {str(float(accuracy_cnt) / len(x))}'</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>我们设置 normalize 为 True 后，函数内部会进行转换，将图像的各个像素除以 255，使得数据的值在 0.0~1.0 之间。先这样把数据限定到某个范围内的处理称为<em>正规化</em>（normalization）。此外，对神经网络的输入数据进行某种既定的转换称为<em>预处理</em>（pre-processing）</p>
</blockquote>
<ul>
<li>预处理很实用。实际上，很多预处理都会考虑到数据的整体分布。比如，利用数据整体的均值或标准差，移动数据，使数据整体以 0 为中心分布，或者进行正规化，把数据延展控制在一定范围内。除此之外，还有将数据整体的分布形状均匀化的方法，即数据<em>白化</em>（whitening）等。</li>
</ul>
<h3 id="3-6-3-批处理"><a href="#3-6-3-批处理" class="headerlink" title="3.6.3 批处理"></a>3.6.3 批处理</h3><blockquote>
<p>现在我们来关注输入数据和权重参数的‘形状’</p>
</blockquote>
<pre class=" language-python"><code class="language-python">x<span class="token punctuation">,</span> _ <span class="token operator">=</span> get_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
network <span class="token operator">=</span> init_network<span class="token punctuation">(</span><span class="token punctuation">)</span>
W1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> W3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W3'</span><span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># (10000, 784)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'x shape: {x.shape}'</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (784,)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'x[0] shape: {x[0].shape}'</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (784, 50)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'w1 shape: {W1.shape}'</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (50, 100)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'w2 shape: {W2.shape}'</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (100, 10)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'w3 shape: {W3.shape}'</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>我们确认了这些多维数组的对应维度的元素个数是一致的（省略了偏置），最终结果也确实是元素个数为 10 的一维数组</p>
</blockquote>
<p><img src="20230829183543.png"></p>
<blockquote>
<p>但是当我们批量进行处理，假设一批有 100 个，则输入的 shape 为(100, 784)，输出形状则为(100,10)，也就是说，输入的 100 张图像的结果被一次性输出了。比如 x[0]和 y[0]中保存了第 0 张图像及其推理结果</p>
</blockquote>
<blockquote>
<p>这种打包式的输入数据称为‘批’(batch)</p>
</blockquote>
<ul>
<li>批处理对计算机的计算大有益处，可以大幅缩短每张图像的处理时间。因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格的讲，相对于数据读入，可以将更多的时间用在计算上）也就是说，批处理一次性计算大型数组比分开逐步计算各个小型数组速度更快</li>
</ul>
<pre class=" language-python"><code class="language-python">x<span class="token punctuation">,</span> t <span class="token operator">=</span> get_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
network <span class="token operator">=</span> init_network<span class="token punctuation">(</span><span class="token punctuation">)</span>

batch_size <span class="token operator">=</span> <span class="token number">100</span>  <span class="token comment" spellcheck="true"># 批数量</span>
accuracy_cnt <span class="token operator">=</span> <span class="token number">0</span>

<span class="token comment" spellcheck="true"># 0~len(x) 每次 i+=batch_size</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x_batch <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i <span class="token operator">+</span> batch_size<span class="token punctuation">]</span>
    y_batch <span class="token operator">=</span> predict<span class="token punctuation">(</span>network<span class="token punctuation">,</span> x_batch<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 在每行找最大值所在列</span>
    p <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_batch<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    accuracy_cnt <span class="token operator">+=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>p <span class="token operator">==</span> t<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i <span class="token operator">+</span> batch_size<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Accuracy: 0.9352</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Accuracy: {str(float(accuracy_cnt) / len(x))}'</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># range的例子</span>
<span class="token comment" spellcheck="true"># [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>list<span class="token punctuation">(</span>range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [0, 3, 6, 9]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>list<span class="token punctuation">(</span>range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>argmax 的例子</p>
</blockquote>
<pre class=" language-python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [1 2 1 0]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>比较结果</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 比较结果</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
t <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [ True  True False  True]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y <span class="token operator">==</span> t<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># y和t相同元素的个数</span>
<span class="token comment" spellcheck="true"># 3</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>y <span class="token operator">==</span> t<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<h2 id="3-7-小结"><a href="#3-7-小结" class="headerlink" title="3.7 小结"></a>3.7 小结</h2><blockquote>
<p>本节介绍了神经网络的前向传播。神经网络和感知机在信号的按层传递上是相同的，但是在向下一个神经元发送信号的时候，改变信号的激活函数有很大差异，神经网络使用的是平滑变化的，而感知机是急剧变化的阶跃函数。</p>
</blockquote>
<ul>
<li>神经网络中的激活函数使用平滑变化的 sigmoid 函数或 ReLU 函数</li>
<li>巧妙利用 Numpy 多维数组，可以高效实现神经网络</li>
<li>机器学习的问题大体上可以分为回归问题和分类问题</li>
<li>关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用 softmax 函数</li>
<li>分类问题中，输出层的神经元的数量设置为要分类的类别数</li>
<li>输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算</li>
</ul>
<h1 id="4-神经网络的学习"><a href="#4-神经网络的学习" class="headerlink" title="4. 神经网络的学习"></a>4. 神经网络的学习</h1><blockquote>
<p>这里说的‘学习’就是指从训练数据中自动获取最优权重参数的过程。为了进行学习，将导入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。为此，我们介绍利用了函数斜率的梯度法。</p>
</blockquote>
<h2 id="4-1-从数据中学习"><a href="#4-1-从数据中学习" class="headerlink" title="4.1 从数据中学习"></a>4.1 从数据中学习</h2><blockquote>
<p>神经网络的特征就是可以从数据中学习。所谓‘从数据中学习’，是指可以由数据自动决定权重参数的值。在实际的神经网络中，参数的数量成千上万，甚至可以达到亿级，如果全部人工指定，那是几乎不可能的。</p>
</blockquote>
<ul>
<li>对于线性可分问题，第 2 章的感知机是可以利用数据自动学习的。根据‘感知机收敛定理’，通过有限次数的学习，线性可分问题是可解的。但是，非线性可分问题则无法通过（自动）学习来解决</li>
</ul>
<h3 id="4-1-1-数据驱动"><a href="#4-1-1-数据驱动" class="headerlink" title="4.1.1 数据驱动"></a>4.1.1 数据驱动</h3><blockquote>
<p>数据是机器学习的核心。通常要解决某个问题，特别是需要发现某种模式时，人们一般会综合考虑各种因素后再给出回答。人们以自己的经验和直觉为线索，通过反复试验推进工作。而机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）。</p>
</blockquote>
<p><img src="20230829194545.png"></p>
<blockquote>
<p>我们来思考从零构建一个能将 5 正确分类的程序，会发现是一个很难的问题。人可以简单地识别出 5，但是却很难明确说出是基于何种规律识别出来的。</p>
</blockquote>
<blockquote>
<p>与其从零开始想一个算法，不如考虑通过有效利用数据来解决这个问题。一种方案是，先从图像中提取<em>特征量</em>，再用机器学习技术学习这些特征量的模式。这里说的‘特征量’是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括 SIFT、SURF 和 HOG 等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的 SVM、KNN 等分类器进行学习。</p>
</blockquote>
<blockquote>
<p>需要注意的是，将图像转换为向量时使用的特征量仍是由人设计的。对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果。比如，为了区分狗的脸部，人们需要考虑与用于识别 5 的特征量不同的其他特征量。即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量。</p>
</blockquote>
<blockquote>
<p>而神经网络则直接学习图像本身。在神经网络中，图像中包含的重要特征量也都是由机器来学习</p>
</blockquote>
<p><img src="20230829194559.png"></p>
<ul>
<li>深度学习有时也称为端到端机器学习(end-to-end machine learning)。<em>端到端</em>指的是从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思</li>
</ul>
<blockquote>
<p>神经网络的优点是对所有的问题都可以用同样的流程来解决。都是通过不断学习所提供的数据，尝试发现待求解的问题的模式。也就是说，与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行‘端到端’的学习</p>
</blockquote>
<h3 id="4-1-2-训练数据和测试数据"><a href="#4-1-2-训练数据和测试数据" class="headerlink" title="4.1.2 训练数据和测试数据"></a>4.1.2 训练数据和测试数据</h3><blockquote>
<p>机器学习中，一般将数据分为<em>训练数据</em>和<em>测试数据</em>两部分来进行学习和实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。而分为两部分数据是因为我们追求的是模型的泛化能力。为了正确评价模型的<em>泛化能力</em>，就必须划分训练数据和测试数据。另外，训练数据也可以称为<em>监督数据</em></p>
</blockquote>
<blockquote>
<p>泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。</p>
</blockquote>
<blockquote>
<p>仅仅用一个数据集去学习和评价参数，是无法进行正确评价的，这样会导致可以顺利处理某个数据集，但无法处理其他数据集的情况。只对某个数据集过度拟合的状态称为<em>过拟合</em>（over fitting）。避免过拟合也是机器学习的一个重要命题</p>
</blockquote>
<h2 id="4-2-损失函数"><a href="#4-2-损失函数" class="headerlink" title="4.2 损失函数"></a>4.2 损失函数</h2><blockquote>
<p>神经网络的学习通过某个指标表示现在的状态，然后，以这个指标为基准，寻找最优权重参数。这个指标称为<em>损失函数</em>（loss function）。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。</p>
</blockquote>
<ul>
<li>损失函数是表示神经网络性能的‘恶劣程度’的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。</li>
</ul>
<h3 id="4-2-1-均方误差"><a href="#4-2-1-均方误差" class="headerlink" title="4.2.1 均方误差"></a>4.2.1 均方误差</h3><p><img src="20230829201218.png"><br><img src="20230829201245.png"></p>
<blockquote>
<p>yk 表示神经网络的输出，tk 表示监督数据，k 表示数据的维数。、</p>
</blockquote>
<blockquote>
<p>比如在之前手写数字识别的例子中，yk、tk 是由如下 10 个元素构成的数据。</p>
</blockquote>
<pre class=" language-python"><code class="language-python">y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span>
t <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
</code></pre>
<blockquote>
<p>这里的 y 是神经网络的输出，是 softmax 函数的结果，可以理解为每个不同分类的概率。而 t 是监督数据，正确标签的值为 1，其他均为 0，这种表示方法称为 <em>one-hot</em> 表示。</p>
</blockquote>
<blockquote>
<p>均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">mean_squared_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">(</span>y <span class="token operator">-</span> t<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>我们来测试一下</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 设'2'为正确解</span>
t <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># 例1: '2'的概率最高的情况</span>
y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span>
mse <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 0.09750000000000003</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>mse<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 例2: '7'的概率最高的情况</span>
y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span>
mse <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 0.5975</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>mse<span class="token punctuation">)</span>
</code></pre>
<h3 id="4-2-2-交叉熵误差-cross-entropy-error"><a href="#4-2-2-交叉熵误差-cross-entropy-error" class="headerlink" title="4.2.2 交叉熵误差(cross entropy error)"></a>4.2.2 交叉熵误差(cross entropy error)</h3><p><img src="20230829202332.png"></p>
<blockquote>
<p>这里，log 表示以 e 为底数的自然对数（log e）。yk 是神经网络的输出，tk 是正确解标签。并且，tk 只有正确解标签的索引为 1，其他均为 0（one-hot 表示）。因此，该式实际上只计算对应正确解标签的输出的自然对数。比如，假设正确解标签的索引是‘2’，对应的神经网络输出是 0.6，则交叉熵误差为-log 0.6 = -0.51。也就是说，交叉熵误差的值是由正确解标签所对应的输出结果决定的。</p>
</blockquote>
<p><img src="20230829202856.png"></p>
<blockquote>
<p>如图所示，x 等于 1 时，y 为 0；随着 x 向 0 靠近，y 逐渐变小。因此，正确解标签对应的输出越大，y 的值越接近 0；当输出为 1 时，交叉熵误差为 0。如果正确解标签对应的输出越小，y 的值就越大</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cross_entropy_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
    delta <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span>
    <span class="token keyword">return</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>t <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y <span class="token operator">+</span> delta<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>这里在计算 log 时，加上了一个微小值 delta，这是因为，当出现 np.log(0)时，会得到负无限大-inf，作为保护性对策，添加一个微小值可以防止负无限大的发生</p>
</blockquote>
<pre class=" language-python"><code class="language-python">t <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>

y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span>
cee <span class="token operator">=</span> cross_entropy_error<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 0.510825457099338</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>cee<span class="token punctuation">)</span>

y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span>
cee <span class="token operator">=</span> cross_entropy_error<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 2.302584092994546</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>cee<span class="token punctuation">)</span>
</code></pre>
<h3 id="4-2-3-mini-batch-学习"><a href="#4-2-3-mini-batch-学习" class="headerlink" title="4.2.3 mini-batch 学习"></a>4.2.3 mini-batch 学习</h3><blockquote>
<p>机器学习使用训练数据进行学习，严格地说，就是针对训练数据计算损失函数的值，找出使值尽可能小的参数。因此，计算损失函数时必须将所有的训练数据作为对象。如果有 100 个训练数据，就要把 100 个损失函数的总和作为学习的指标。</p>
</blockquote>
<blockquote>
<p>前面的例子都是针对单个数据的损失函数。如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以改写为：</p>
</blockquote>
<p><img src="20230829205439.png"></p>
<blockquote>
<p>假设数据有 N 个，tnk 表示第 n 个数据的第 k 个元素的值（ynk 是神经网络的输出，tnk 是监督数据）。这里其实是把求单个数据的损失函数的式子扩大到了 N 份数据，最后还要除以 N 进行正规化。通过除以 N，可以求单个数据的‘平均损失函数’。通过这样的正规化，可以获得和训练数据的数量无关的统一指标。即使有 1000、10000 个数据，也能求单个数据的平均损失函数。</p>
</blockquote>
<blockquote>
<p>MNIST 数据集的训练数据有 60000 个，如果以全部数据为对象求损失函数的和，则计算过程需要花费较长的时间（大数据集同理）。这种情况下，以全部数据为对象计算损失函数是不现实的。因此，我们从全部数据中选出一批数据（称为 mini-batch，小批量），然后对每个 mini-batch 进行学习。这种方式称为<em>mini-batch</em>学习</p>
</blockquote>
<blockquote>
<p>下面编写从训练数据中随机选择指定个数的数据的代码</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>pardir<span class="token punctuation">)</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist

<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> \
    load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> one_hot_label<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># (60000, 784)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># (60000, 10)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>使用 np.random.choice()随机抽取 10 笔数据</p>
</blockquote>
<pre class=" language-python"><code class="language-python">train_size <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
batch_size <span class="token operator">=</span> <span class="token number">10</span>
batch_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>train_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>
t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>
</code></pre>
<blockquote>
<p>使用 np.random.choice 可以从指定的数字中随机选择想要的数字。比如 np.random.choice(60000,10)会从 0~59999 之间随机选择 10 个数字。我们可以得到一个包含被选数据的索引的数组。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [13591 30046 50818  9014 19622 34605  3242 19230 41399 14322]</span>
</code></pre>
<blockquote>
<p>我们只需指定这些随机选出的索引，取出 mini-batch，然后使用 mini-batch 计算损失函数即可。</p>
</blockquote>
<ul>
<li>mini-batch 的损失函数也是利用一部分样本数据来近似地计算整体。也就是说，用随机选择的小批量数据作为全体训练数据的近似值</li>
</ul>
<h3 id="4-2-4-mini-batch-版交叉熵误差的实现"><a href="#4-2-4-mini-batch-版交叉熵误差的实现" class="headerlink" title="4.2.4 mini-batch 版交叉熵误差的实现"></a>4.2.4 mini-batch 版交叉熵误差的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cross_entropy_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> y<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        t <span class="token operator">=</span> t<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">.</span>size<span class="token punctuation">)</span>
        y <span class="token operator">=</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>size<span class="token punctuation">)</span>

    batch_size <span class="token operator">=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>t <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> batch_size
</code></pre>
<blockquote>
<p>y 是神经网络的输出，t 是监督数据。y 的维度为 1 时，即求单个数据的交叉熵误差时，需要改变数据的形状。并且，当输入为 mini-batch 时，要用 batch 的个数进行正规化，计算单个函数的平均交叉熵误差</p>
</blockquote>
<blockquote>
<p>当监督数据是标签现实（非 one-hot 表示，而是像‘2’、‘7’这种标签），交叉熵误差函数可以改为：</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cross_entropy_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> y<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        t <span class="token operator">=</span> t<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">.</span>size<span class="token punctuation">)</span>
        y <span class="token operator">=</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>size<span class="token punctuation">)</span>

    batch_size <span class="token operator">=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y<span class="token punctuation">[</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">,</span> t<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> batch_size
</code></pre>
<blockquote>
<p>实现的要点是，由于 one-hot 表示中 t 为 0 的元素的交叉熵误差为 0，因此针对这些元素的计算可以忽略。换言之，如果可以获得神经网络在正确解标签的输出，就可以计算交叉熵误差。因此，t 为 one-hot 表示时通过 t*np.log(y)计算的地方，在 t 为标签形式时，可用 np.log(y[np.arange(batch_size), t])实现相同的处理。</p>
</blockquote>
<blockquote>
<p>np.arange(batch_size)会生成一个从 0 到 batch_size-1 的数组。比如当 batch_szie 为 5，则生成[0,1,2,3,4]，而 t 中标签是以[2,7,0,9,4]的形式存储的，所以 y[np.arange(batch_size), t]会生成 numpy 数组[y[0,2],y[1,7],y[2,0],y[3,9],y[4,4],]</p>
</blockquote>
<h3 id="4-2-5-为何要设定损失函数"><a href="#4-2-5-为何要设定损失函数" class="headerlink" title="4.2.5 为何要设定损失函数"></a>4.2.5 为何要设定损失函数</h3><blockquote>
<p>以数字识别任务为例，为什么我们要引入一个损失函数，而不是直接以识别精度为指标呢？</p>
</blockquote>
<blockquote>
<p>可以根据‘导数’在神经网络学习中的作用来回答。寻找最优参数时（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。为此，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值。</p>
</blockquote>
<blockquote>
<p>假设有一个神经网络，对于其中的某一个权重参数。此时，对该权重参数的损失函数求导，表示的是‘如果稍微改变这个权重参数的值，损失函数的值会如何变化’。如果导数的值为负，通过使该权重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值。不过，当导数的值为 0 时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处</p>
</blockquote>
<blockquote>
<p>之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数都会变为 0，导致参数无法更新。</p>
</blockquote>
<pre><code>在进行神经网络的学习时，不能讲识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0
</code></pre>
<blockquote>
<p>假设某个神经网络正确识别了 100 笔训练数据中的 32 笔，此时识别精度为 32%。如果以识别精度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32%，不会出现变化。也就是说，仅仅微调参数，是无法改善识别精度的。即使有所改善，也不会像 32.0123…%这样连续变化，而是变为 33%、34%这样的不连续的、离散的值。而如果把损失函数作为指标，则当前损失函数的值可以表示为 0.92543…这样的值。并且，如果稍微改变一下参数的值，对应的损失函数也会像 0.93432…这样发生连续的变化。</p>
</blockquote>
<blockquote>
<p>识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。</p>
</blockquote>
<p><img src="20230830150042.png"></p>
<h2 id="4-3-数值微分"><a href="#4-3-数值微分" class="headerlink" title="4.3 数值微分"></a>4.3 数值微分</h2><blockquote>
<p>梯度法使用梯度的信息决定前进的方向</p>
</blockquote>
<h3 id="4-3-1-导数"><a href="#4-3-1-导数" class="headerlink" title="4.3.1 导数"></a>4.3.1 导数</h3><blockquote>
<p>加入你十分钟内跑了 2 千米。如果要计算此时的奔跑速度，则为 2/10 = 0.2[千米/分]。也就是说，以 1 分钟前进 0.2 千米的速度（变化）奔跑</p>
</blockquote>
<blockquote>
<p>这个例子中，我们计算了‘奔跑的距离’相对于‘时间’发生了多大变化。但是，严格地说，这个计算方式计算的是 10 分钟内的平均速度。而导数表示的是某个瞬间的变化量。因此，将 10 分钟的这一时间段尽可能地缩短，比如计算前 1 分钟奔跑的距离、前 1 秒钟奔跑的距离、前 0.1 秒奔跑的距离……就可以获得某个瞬间的变化量（某个瞬间速度）</p>
</blockquote>
<blockquote>
<p>综上，导数就是表示某个瞬间的变化量。</p>
</blockquote>
<p><img src="20230830150707.png"></p>
<blockquote>
<p>d f(x) / dx 表示 f(x)关于 x 的导数，即 f(x)相对于 x 的变化程度。这个式子表示的导数的含义是，x 的‘微小变化’将导致函数 f(x)的值在多大程度上发生变化。其中，表示微小变化的 h 无限趋近 0。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 不好的实现</span>
<span class="token keyword">def</span> <span class="token function">numerical_diff</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    h <span class="token operator">=</span> <span class="token number">10e</span><span class="token operator">-</span><span class="token number">50</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>f<span class="token punctuation">(</span>x <span class="token operator">+</span> h<span class="token punctuation">)</span> <span class="token operator">-</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> h
</code></pre>
<blockquote>
<p>该函数的名称来源于<em>数值微分</em>的英文 numerical differentiation。这个函数有两个参数，即‘函数 f’和‘传给函数 f 的参数 x’。看似没问题，实际上有两处需要改变的地方</p>
</blockquote>
<blockquote>
<p>在上面的实现中，因为想把尽可能小的值赋给 h（无限接近 0），所以使用了 10e-50 这个微小值。但是，反而产生了<em>舍入误差</em>。舍入误差就是指，因为省略小数的精细部分的数值（比如，小数点后第 8 位以后的数值）而造成最终的计算结果上的误差。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 0.0</span>
</code></pre>
<blockquote>
<p>如果用 float32 类型（32 位的浮点数）来表示 1e-50，就会变成 0.0，无法正确表示出来。也就是说，使用过小的值会造成计算机出现计算上的问题。所以这里进行改进，将微小值 h 改为 10^-4</p>
</blockquote>
<blockquote>
<p>第二个要改进的地方和函数 f 的差分有关。虽然上述实现中计算了函数 f 在 x+h 和 x 之间的差分，但是，这个计算从一开始就有误差。如图 4-5 所示，‘真的导数’对应函数在 x 处的斜率（称为切线），但是上述实现中计算的是(x+h)和 x 之间的斜率。因此，真的导数（真的切线）和上述实现中得到的导数的值在严格意义上并不一致。这个差异的出现是因为 h 不可能无限接近 0</p>
</blockquote>
<blockquote>
<p>为例减少这个误差，我们可以计算函数 f 在(x+h)和(x-h)之间的差分。因为这种计算方法以 x 为中心，计算它左右两边的差分，所以也称为<em>中心差分</em>（而(x+h)和 x 之间的差分称为<em>前向差分</em>）。</p>
</blockquote>
<p><img src="20230830153418.png"></p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">numerical_diff</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    h <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span>  <span class="token comment" spellcheck="true"># 0.0001</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>f<span class="token punctuation">(</span>x <span class="token operator">+</span> h<span class="token punctuation">)</span> <span class="token operator">-</span> f<span class="token punctuation">(</span>x <span class="token operator">-</span> h<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> h<span class="token punctuation">)</span>
</code></pre>
<ul>
<li>利用微小的差分求导数的过程称为<em>数值微分</em>（numerical_differentiation）。而基于数学式的推导求导数的过程，则用<em>解析性</em>（analytic）一词，称为‘解析性求解’或‘解析性求导’。比如 y=x^2 的导数，可以通过 dy/dx=2x 解析性地求出来。解析性求导得到的导数是不含误差的‘真的导数’</li>
</ul>
<h3 id="4-3-2-数值微分的例子"><a href="#4-3-2-数值微分的例子" class="headerlink" title="4.3.2 数值微分的例子"></a>4.3.2 数值微分的例子</h3><blockquote>
<p>试着用数值微分对简单函数进行求导。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">function_1</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">0.01</span> <span class="token operator">*</span> x <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">0.1</span> <span class="token operator">*</span> x
</code></pre>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pylab <span class="token keyword">as</span> plt

plt<span class="token punctuation">.</span>switch_backend<span class="token punctuation">(</span><span class="token string">'TkAgg'</span><span class="token punctuation">)</span>

x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">20.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 以0.1为单位，从0到20的数组x</span>
y <span class="token operator">=</span> function_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'f(x)'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="20230830155930.png"></p>
<blockquote>
<p>计算它在 x=5 和 x=10 处的导数</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 0.1999999999990898</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>numerical_diff<span class="token punctuation">(</span>function_1<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 0.2999999999986347</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>numerical_diff<span class="token punctuation">(</span>function_1<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>这里计算的导数是 f(x)相对于 x 的变化量，对应函数的斜率。另外 f(x)=0.01x^2+0.1x 的解析解是 d f(x) / dx = 0.02x + 0.1。因此，在 x=5 和 x=10 处，‘真的函数’分别为 0.2 和 0.3，我们计算的结果和它不一致，但是误差非常小，可以看作相等</p>
</blockquote>
<blockquote>
<p>用上面的数值微分的值作为斜率，画一条直线。可以确认这些直线确实对应函数的切线</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># coding: utf-8</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pylab <span class="token keyword">as</span> plt


<span class="token keyword">def</span> <span class="token function">numerical_diff</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    h <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span> <span class="token comment" spellcheck="true"># 0.0001</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>f<span class="token punctuation">(</span>x<span class="token operator">+</span>h<span class="token punctuation">)</span> <span class="token operator">-</span> f<span class="token punctuation">(</span>x<span class="token operator">-</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>h<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">function_1</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">0.01</span><span class="token operator">*</span>x<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token number">0.1</span><span class="token operator">*</span>x


<span class="token keyword">def</span> <span class="token function">tangent_line</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    d <span class="token operator">=</span> numerical_diff<span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span>
    y <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">-</span> d<span class="token operator">*</span>x
    <span class="token keyword">return</span> <span class="token keyword">lambda</span> t<span class="token punctuation">:</span> d<span class="token operator">*</span>t <span class="token operator">+</span> y

x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">20.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> function_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"x"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"f(x)"</span><span class="token punctuation">)</span>

tf <span class="token operator">=</span> tangent_line<span class="token punctuation">(</span>function_1<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
y2 <span class="token operator">=</span> tf<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y2<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="20230830161218.png"></p>
<h3 id="4-3-3-偏导数"><a href="#4-3-3-偏导数" class="headerlink" title="4.3.3 偏导数"></a>4.3.3 偏导数</h3><blockquote>
<p>下面看一个计算参数的平方和的简单函数</p>
</blockquote>
<p><img src="20230830161442.png"></p>
<blockquote>
<p>我们假定向参数输入了一个 Numpy 数组，画一下图像。</p>
</blockquote>
<p><img src="20230830162015.png"></p>
<blockquote>
<p>因为该函数有多个变量，所以求导时要区分对哪个变量求导数，有多个变量的函数的导数称为<em>偏导数</em>。数学表式可以写成 əf/əx0、əf/əx1</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># x0=3 x1=4 求关于x0的偏导</span>
<span class="token keyword">def</span> <span class="token function">function_tmp1</span><span class="token punctuation">(</span>x0<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x0 <span class="token operator">*</span> x0 <span class="token operator">+</span> <span class="token number">4.0</span> <span class="token operator">**</span> <span class="token number">2.0</span>


<span class="token comment" spellcheck="true"># 6.00000000000378</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>numerical_diff<span class="token punctuation">(</span>function_tmp1<span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token comment" spellcheck="true"># x0=3 x1=4 求关于x1的偏导</span>
<span class="token keyword">def</span> <span class="token function">function_tmp2</span><span class="token punctuation">(</span>x1<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">3.0</span> <span class="token operator">**</span> <span class="token number">2.0</span> <span class="token operator">+</span> x1 <span class="token operator">*</span> x1


<span class="token comment" spellcheck="true"># 7.999999999999119</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>numerical_diff<span class="token punctuation">(</span>function_tmp2<span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>偏导数额和单变量的导数一样，都是求某个地方的斜率。不管，偏导数需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。上述代码中，为了将目标变量以外的变量固定到某些特定的值上，我们定义了新的函数。如何，对新定义的函数应用了之前的求数值微分的函数，得到偏导数。</p>
</blockquote>
<h2 id="4-4-梯度"><a href="#4-4-梯度" class="headerlink" title="4.4 梯度"></a>4.4 梯度</h2><blockquote>
<p>上面我们分别计算了 x0 和 x1 的偏导数。现在，考虑求 x0=3、x1=4 时(x0,x1)的偏导数(əf/əx0,əf/əx1)。像这样由全部变量的偏导数汇总而成的向量称为<em>梯度</em>（gradient）。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">numerical_gradient</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    h <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span>  <span class="token comment" spellcheck="true"># 0.0001</span>
    grad <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 生成和x形状相同的数组</span>

    <span class="token keyword">for</span> idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        tmp_val <span class="token operator">=</span> x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
        <span class="token comment" spellcheck="true"># 分别对x[idx]+h和x[idx]-h进行求导（x其他项不变）</span>
        <span class="token comment" spellcheck="true"># f(x+h)</span>
        x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tmp_val <span class="token operator">+</span> h
        fxh1 <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># f(x-h)</span>
        x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tmp_val <span class="token operator">-</span> h
        fxh2 <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        grad<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>fxh1 <span class="token operator">-</span> fxh2<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> h<span class="token punctuation">)</span>
        x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tmp_val  <span class="token comment" spellcheck="true"># 还原值</span>

    <span class="token keyword">return</span> grad
</code></pre>
<blockquote>
<p>下面用该函数计算梯度</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># [6. 8.]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>numerical_gradient<span class="token punctuation">(</span>function_2<span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [0. 4.]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>numerical_gradient<span class="token punctuation">(</span>function_2<span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [6. 0.]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>numerical_gradient<span class="token punctuation">(</span>function_2<span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>这些梯度意味着什么呢？为了理解，我们将 f(x0,x1)=x0^2+x1^2 的梯度画在图上。不过这里画的是元素值为负梯度的向量</p>
</blockquote>
<p><img src="20230830164155.png"></p>
<blockquote>
<p>可以看到，该函数的梯度呈现为有向向量（箭头）。并且梯度指向函数的最低处（最小值），所有的箭头都指向同一点。其次，我们发现，离‘最低处’越远，箭头越大</p>
</blockquote>
<blockquote>
<p>实际上，梯度会指向各点处的函数值降低的方向。更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向。这是一个非常重要的性质</p>
</blockquote>
<h3 id="4-4-1-梯度法"><a href="#4-4-1-梯度法" class="headerlink" title="4.4.1 梯度法"></a>4.4.1 梯度法</h3><blockquote>
<p>机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。这里说的最优参数是指损失函数取最小值的参数。一般而言，损失函数很复杂，参数空间庞大。不知道何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。</p>
</blockquote>
<blockquote>
<p>需要注意的是，梯度表示的是各点处的函数值减小最多的方向。因此，无法保证梯度所指的方向就是函数的最小值或真正应该前进的方向。实际上，复杂的函数中，梯度指示的方向基本上都不是函数值最小处</p>
</blockquote>
<ul>
<li>函数的极小值、最小值以及被称为<em>鞍点</em>（saddle point）的地方，梯度为 0。极小值是局部最小值，是限定在某个范围内的最小值。鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。梯度法就是要寻找梯度为 0 的地方，当不一定会找到最小值（也可能是极小值或鞍点）。此外，当函数很复杂且呈扁平状的时候，学习可能会进入一个（几乎）平坦的地区，陷入被称为‘学习高原’的无法前进的停滞期。</li>
</ul>
<blockquote>
<p>虽然梯度的方向不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值。因此，在寻找函数的最小值时，要以梯度的信息为线索，决定前进的方向。</p>
</blockquote>
<blockquote>
<p>梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断沿着梯度方向前进。像这样，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是<em>梯度法</em>（gradient method）。梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络中经常使用。</p>
</blockquote>
<ul>
<li>根据目的是寻找最小值还是最大值，梯度法分为：寻找最小值的<em>梯度下降法</em>（gradient descent method），寻找最大值的<em>梯度上升法</em>（gradient ascent method）。但是通过反转损失函数的符号，求最大或最小值的问题可以变成一样的问题，上升或下降的差异本质上不重要。一般来说，在神经网络（深度学习）中，梯度法主要是指梯度下降法</li>
</ul>
<blockquote>
<p>用数学式表示梯度</p>
</blockquote>
<p><img src="20230830183501.png"></p>
<blockquote>
<p>η 表示更新量，在神经网络中，称为<em>学习率</em>（learning rate）。学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数</p>
</blockquote>
<blockquote>
<p>该式是表示更新一次的式子，这个步骤会反复执行。每一步都按该式更新变量的值，通过反复执行此步骤，逐渐减少函数值。即使是多个变量，也可以通过类似的式子（各个变量的偏导数）进行更新</p>
</blockquote>
<blockquote>
<p>学习率需要事先确定为某个值，一般而言，这个值过大或过小，都无法抵达一个‘好的位置’。在神经网络学习中，一般会一边改变学习率的值，一边确定学习是否正确进行了</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">gradient_descent</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> init_x<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> step_num<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> init_x

    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>step_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
        grad <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
        x <span class="token operator">-=</span> lr <span class="token operator">*</span> grad

    <span class="token keyword">return</span> x
</code></pre>
<blockquote>
<p>参数 f 是要进行最优化的参数，init_x 是初始值，lr 是学习率，step_num 是梯度法的重复次数</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">function_2</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">**</span> <span class="token number">2</span>


init_x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
res <span class="token operator">=</span> gradient_descent<span class="token punctuation">(</span>function_2<span class="token punctuation">,</span> init_x<span class="token operator">=</span>init_x<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> step_num<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [-6.11110793e-10  8.14814391e-10]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>这个结果非常接近真实的最小值(0,0)。如果用图来表示梯度法的更新过程，可以发现，原点处是最低的地方，函数的取值一点点在向其靠近。</p>
</blockquote>
<p><img src="20230830184632.png"></p>
<blockquote>
<p>学习率过大或过小都无法得到好的结果。下面实验一下</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 学习率过大的例子 lr=10.0</span>
init_x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
res <span class="token operator">=</span> gradient_descent<span class="token punctuation">(</span>function_2<span class="token punctuation">,</span> init_x<span class="token operator">=</span>init_x<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">10.0</span><span class="token punctuation">,</span> step_num<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [-2.58983747e+13 -1.29524862e+12]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 学习率过小的例子</span>
init_x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
res <span class="token operator">=</span> gradient_descent<span class="token punctuation">(</span>function_2<span class="token punctuation">,</span> init_x<span class="token operator">=</span>init_x<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span> step_num<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [-2.99999994  3.99999992]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>实验结果表明，学习率过大的话，会发散成一个很大的值；反过来，学习率过小的话，基本上没怎么更新就结束了</p>
</blockquote>
<ul>
<li>像学习率这样的参数称为<em>超参数</em>。和神经网络的参数（权重和偏置）性质不同。神经网络的参数是通过训练数据和学习算法自动获得的，学习率这种超参数则是人工设定的。</li>
</ul>
<h3 id="4-4-2-神经网络的梯度"><a href="#4-4-2-神经网络的梯度" class="headerlink" title="4.4.2 神经网络的梯度"></a>4.4.2 神经网络的梯度</h3><blockquote>
<p>神经网络的学习也要求梯度。这里说的梯度是指损失函数关于权重参数的梯度。</p>
</blockquote>
<blockquote>
<p>损失函数用 L 表示，权重用 W 表示，则梯度用 əL/əW 表示</p>
</blockquote>
<p><img src="20230830194205.png"></p>
<blockquote>
<p>əL/əW 的元素由各个元素关于 W 的偏导数构成。表示当 W 变化时，损失函数 L 会发生多大变化，重点是，əL/əW 的形状和 W 相同。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">simpleNet</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 高斯分布进行初始化</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        z <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> cross_entropy_error<span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

        <span class="token keyword">return</span> loss
</code></pre>
<blockquote>
<p>simpleNet 类只有一个实例变量，即 2x3 的权重参数，有两个方法，一个用于预测，一个用于计算损失函数。参数 x 接收输入数据，参数 t 接收正确解标签</p>
</blockquote>
<pre class=" language-python"><code class="language-python">net <span class="token operator">=</span> simpleNet<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [[-0.26117395  0.46609188  0.42375899]</span>
<span class="token comment" spellcheck="true">#  [ 0.57508072 -0.74113595 -0.15255521]]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>W<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 权重参数</span>

x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
p <span class="token operator">=</span> net<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [ 0.36086827 -0.38736723  0.11695571]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span>

max <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>p<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 最大值索引</span>
<span class="token comment" spellcheck="true"># 0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>max<span class="token punctuation">)</span>

t <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 1.0578410655701647</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>接下来求梯度。这里定义的函数 f(W)的参数 W 是一个伪参数。因为 numerical_gradient(f,x)会在内部执行 f(x)，为了与之兼容而定义 f(W)包裹实际要算的 net.loss(x,t)和 W 的偏导</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>W<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> net<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>


dW <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>f<span class="token punctuation">,</span> net<span class="token punctuation">.</span>W<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [[ 0.41552689  0.01403747 -0.42956436]</span>
<span class="token comment" spellcheck="true">#  [ 0.62329034  0.0210562  -0.64434654]]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dW<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>观察 əL/əW 中的 əL/əW11 的值大概是 0.4，这表示，如果 w11 增加 h，则损失函数的值会增加 0.2h。再看 əL/əW23 大概是-0.6，这表示，如果 w23 增加 h，则损失函数的值会减少 0.6h。从减小损失函数值的观点来看，w23 应向正方向更新，w11 应向负方向更新。至于更新的程度，w23 比 w11 的贡献大</p>
</blockquote>
<blockquote>
<p>上述代码定义函数使用了 def f(x)…，在 python 中，如果定义的是简单的函数，可以使用 lambda 表示法。</p>
</blockquote>
<pre class=" language-python"><code class="language-python">f <span class="token operator">=</span> <span class="token keyword">lambda</span> w<span class="token punctuation">:</span> net<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
dW <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>f<span class="token punctuation">,</span> net<span class="token punctuation">.</span>W<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可。</p>
</blockquote>
<ul>
<li>为了对应形状为多维数组的权重参数 w，这里使用的 numerical_gradient 和之前的实现稍有不同。</li>
</ul>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">numerical_gradient</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    h <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span> <span class="token comment" spellcheck="true"># 0.0001</span>
    grad <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

    it <span class="token operator">=</span> np<span class="token punctuation">.</span>nditer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> flags<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'multi_index'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> op_flags<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'readwrite'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">while</span> <span class="token operator">not</span> it<span class="token punctuation">.</span>finished<span class="token punctuation">:</span>
        idx <span class="token operator">=</span> it<span class="token punctuation">.</span>multi_index
        tmp_val <span class="token operator">=</span> x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
        x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> float<span class="token punctuation">(</span>tmp_val<span class="token punctuation">)</span> <span class="token operator">+</span> h
        fxh1 <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># f(x+h)</span>

        x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tmp_val <span class="token operator">-</span> h
        fxh2 <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># f(x-h)</span>
        grad<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>fxh1 <span class="token operator">-</span> fxh2<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>h<span class="token punctuation">)</span>

        x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tmp_val <span class="token comment" spellcheck="true"># 还原值</span>
        it<span class="token punctuation">.</span>iternext<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> grad
</code></pre>
<h2 id="4-5-学习算法的实现"><a href="#4-5-学习算法的实现" class="headerlink" title="4.5 学习算法的实现"></a>4.5 学习算法的实现</h2><blockquote>
<p>神经网络的学习步骤</p>
</blockquote>
<ul>
<li><p>前提<br>神经网络查找合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为‘学习’。神经网络的学习分为下面 4 个步骤：</p>
</li>
<li><p>步骤 1（mini-batch）<br>从训练数据中随机选出一部分数据，这部分数据称为 mini-batch。目标是减少 mini-batch 的损失函数的值。</p>
</li>
<li><p>步骤 2（计算梯度）<br>为了减少 mini-batch 的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减少最多的方向。</p>
</li>
<li><p>步骤 3（更新参数）<br>将权重参数沿梯度方向进行微小更新。</p>
</li>
<li><p>步骤 4（重复）<br>重复步骤 1、2、3</p>
</li>
</ul>
<blockquote>
<p>神经网络的学习按照上面四个步骤。这个方法通过梯度下降法更新参数，因为这里使用的数据是随机选择的 mini batch 数据，所以又称为<em>随机梯度下降法</em>（stochastic gradient descent）。‘随机’是随机选择的意思，随机梯度下降就是‘对随机选择的数据进行梯度下降法’。深度学习的很多框架中，实现随机梯度下降法的函数一般用名为 SGD。</p>
</blockquote>
<h3 id="4-5-1-2-层神经网络的类"><a href="#4-5-1-2-层神经网络的类" class="headerlink" title="4.5.1 2 层神经网络的类"></a>4.5.1 2 层神经网络的类</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os

<span class="token comment" spellcheck="true"># 拼接父目录，接下来就可以从父目录导入python依赖</span>
<span class="token comment" spellcheck="true"># sys.path.append(os.pardir)</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">from</span> common<span class="token punctuation">.</span>functions <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> common<span class="token punctuation">.</span>gradient <span class="token keyword">import</span> numerical_gradient


<span class="token keyword">class</span> <span class="token class-name">TwoLayerNet</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span>
                 output_size<span class="token punctuation">,</span> weight_init_std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 初始化权重</span>
        self<span class="token punctuation">.</span>params <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> \
                            np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> \
                            np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>output_size<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        W1<span class="token punctuation">,</span> W2 <span class="token operator">=</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span>
        b1<span class="token punctuation">,</span> b2 <span class="token operator">=</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span>

        a1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W1<span class="token punctuation">)</span> <span class="token operator">+</span> b1
        z1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a1<span class="token punctuation">)</span>
        a2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z1<span class="token punctuation">,</span> W2<span class="token punctuation">)</span> <span class="token operator">+</span> b2
        y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>a2<span class="token punctuation">)</span>

        <span class="token keyword">return</span> y

    <span class="token comment" spellcheck="true"># x：输入数据，t：监督数据</span>
    <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> cross_entropy_error<span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">accuracy</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        t <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        accuracy <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>y <span class="token operator">==</span> t<span class="token punctuation">)</span> <span class="token operator">/</span> float<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> accuracy

    <span class="token comment" spellcheck="true"># x：输入数据，t：监督数据</span>
    <span class="token keyword">def</span> <span class="token function">numerical_gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss_W <span class="token operator">=</span> <span class="token keyword">lambda</span> W<span class="token punctuation">:</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

        grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        grads<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> grads
</code></pre>
<p><img src="20230830210728.png"></p>
<pre class=" language-python"><code class="language-python">net <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (784, 100)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (100,)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (100, 10)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (10,)</span>
</code></pre>
<blockquote>
<p>params 变量中保存了该神经网络所需的全部参数。并且这些权重参数会用在推理处理（前向处理）中。推理处理的实现如下：</p>
</blockquote>
<pre class=" language-python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 伪输入数据（100笔）</span>
y <span class="token operator">=</span> net<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>grads 变量中保存了各个参数的梯度。计算完梯度后，梯度的信息将保存在 grads 变量中。</p>
</blockquote>
<pre class=" language-python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 伪输入数据（100笔）</span>
t <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 伪正确解标签（100笔）</span>

grads <span class="token operator">=</span> net<span class="token punctuation">.</span>numerical_gradient<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 计算梯度</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (784, 100)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (100,)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (100, 10)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (10,)</span>
</code></pre>
<blockquote>
<p>hidden_size 是隐藏层的神经元数，设置为一个合适的值即可。input_size=784 是因为 MNIST 数据集的图像是 28x28 像素的，输出是 10 个类别，所以 output_size=10</p>
</blockquote>
<blockquote>
<p>初始化方法会对权重参数进行初始化。如何设置权重参数的初始值是关系到神经网络能否成功学习的重要问题。这里使用符合高斯分布的随机数初始化权重参数，使用 0 初始化偏置</p>
</blockquote>
<blockquote>
<p>numerical_gradient 方法基于数值微分计算各个参数相对于损失函数的梯度。gradient(self,x,t)是下一章要实现的算法，使用误差反向传播法高效地计算梯度</p>
</blockquote>
<h3 id="4-5-2-mini-batch-的实现"><a href="#4-5-2-mini-batch-的实现" class="headerlink" title="4.5.2 mini-batch 的实现"></a>4.5.2 mini-batch 的实现</h3><blockquote>
<p>mini batch 方法是从训练数据中随机选择一部分数据，再以这些 mini-batch 为对象，使用梯度法更新参数的过程。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist
<span class="token keyword">from</span> two_layer_net <span class="token keyword">import</span> TwoLayerNet

<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> \
    load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> one_hot_label<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

train_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># 超参数</span>
iters_num <span class="token operator">=</span> <span class="token number">10000</span>
train_size <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
batch_size <span class="token operator">=</span> <span class="token number">100</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>
network <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>iters_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 获取mini-batch</span>
    <span class="token comment" spellcheck="true"># 从train_size中随机选batch_size个数字</span>
    batch_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>train_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>
    t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># 计算梯度</span>
    grad <span class="token operator">=</span> network<span class="token punctuation">.</span>numerical_gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># grad = network.gradient(x_batch, t_batch) # 高速版</span>

    <span class="token comment" spellcheck="true"># 更新参数</span>
    <span class="token keyword">for</span> key <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'W1'</span><span class="token punctuation">,</span> <span class="token string">'b1'</span><span class="token punctuation">,</span> <span class="token string">'W2'</span><span class="token punctuation">,</span> <span class="token string">'b2'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        network<span class="token punctuation">.</span>params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> grad<span class="token punctuation">[</span>key<span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># 记录学习过程</span>
    loss <span class="token operator">=</span> network<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
    train_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>用图像来表示损失函数的值的推移</p>
</blockquote>
<p><img src="20230830213225.png"></p>
<blockquote>
<p>可以看出，随着学习的进行，损失函数的值在不断减小，这是学习正常进行的信号，表示神经网络的权重参数在逐渐拟合数据。</p>
</blockquote>
<h3 id="4-5-3-基于测试数据的评价"><a href="#4-5-3-基于测试数据的评价" class="headerlink" title="4.5.3 基于测试数据的评价"></a>4.5.3 基于测试数据的评价</h3><blockquote>
<p>之前的学习，计算了损失函数，但是严格地说是‘对训练数据的某个 mini-batch 的损失函数’的值。训练数据的损失函数值减小，光看这个结果还不能说明该神经网络在其他数据集上也一定能有同等程度的表现。</p>
</blockquote>
<blockquote>
<p>神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数据，即确认是否会发生过拟合。过拟合是指，虽然训练数据中的数字图像能够正确识别，但是不在训练数据中的数字图像却无法被识别的现象。</p>
</blockquote>
<blockquote>
<p>神经网络学习的最初目标是掌握泛化能力，为此，需要使用不包含在训练数据中的数据。下面的代码在进行学习的过程中，会定期对训练数据和测试数据记录识别精度。这里，每经过一个 epoch，都会记录下训练数据和测试数据的识别精度。</p>
</blockquote>
<ul>
<li>epoch 是一个单位。一个 epoch 表示学习中所有训练数据均被使用过一次的更新次数。比如 10000 笔训练数据，用大小为 100 笔数据的 mini-batch 进行学习时，重复随机梯度下降法 100 次，所有的训练数据就都被‘看过’了。此时，100 此就是一个 epoch</li>
</ul>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist
<span class="token keyword">from</span> two_layer_net <span class="token keyword">import</span> TwoLayerNet

<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> \
    load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> one_hot_label<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

train_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token comment" spellcheck="true"># 训练和测试时的识别精度</span>
train_acc_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
test_acc_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token comment" spellcheck="true"># 平均每个epoch的重复次数</span>
iter_per_epoch <span class="token operator">=</span> \
    max<span class="token punctuation">(</span>train_size <span class="token operator">/</span> batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1和train_size / batch_size中最大的那个</span>

<span class="token comment" spellcheck="true"># 超参数</span>
iters_num <span class="token operator">=</span> <span class="token number">10000</span>
train_size <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
batch_size <span class="token operator">=</span> <span class="token number">100</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>
network <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>iters_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 获取mini-batch</span>
    <span class="token comment" spellcheck="true"># 从train_size中随机选batch_size个数字</span>
    batch_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>train_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>
    t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># 计算梯度</span>
    grad <span class="token operator">=</span> network<span class="token punctuation">.</span>numerical_gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># grad = network.gradient(x_batch, t_batch) # 高速版</span>

    <span class="token comment" spellcheck="true"># 更新参数</span>
    <span class="token keyword">for</span> key <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'W1'</span><span class="token punctuation">,</span> <span class="token string">'b1'</span><span class="token punctuation">,</span> <span class="token string">'W2'</span><span class="token punctuation">,</span> <span class="token string">'b2'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        network<span class="token punctuation">.</span>params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> grad<span class="token punctuation">[</span>key<span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># 记录学习过程</span>
    loss <span class="token operator">=</span> network<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
    train_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 计算每个epoch的识别精度</span>
    <span class="token keyword">if</span> i <span class="token operator">%</span> iter_per_epoch <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        train_acc <span class="token operator">=</span> network<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span>
        test_acc <span class="token operator">=</span> network<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span>
        train_acc_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_acc<span class="token punctuation">)</span>
        test_acc_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>test_acc<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'train acc, test acc | {str(train_acc)}, {str(test_acc)}'</span><span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>之所以计算的是每个 epoch 的精度，是因为如果每个 for 都计算，会消耗很多性能，而且也没必要那么频繁地计算识别精度</p>
</blockquote>
<p><img src="20230830221443.png"></p>
<h2 id="4-6-小结"><a href="#4-6-小结" class="headerlink" title="4.6 小结"></a>4.6 小结</h2><ul>
<li>机器学习中使用的数据集分为训练数据和测试数据</li>
<li>神经网络用训练数据进行学习，并用测试数据评价学习到的模型的泛化能力</li>
<li>神经网络的学习以损失函数为指标，更新权重参数，以使损失函数的值减小</li>
<li>利用某个给定的微小值的差分求导数的过程，称为数值微分</li>
<li>利用数值微分，可以计算权重参数的梯度</li>
<li>数值微分虽然费时间，但是实现起来很简单。下一章中要实现的稍微复杂一些的误差反向传播法可以高速地计算梯度</li>
</ul>
<h1 id="5-误差反向传播法"><a href="#5-误差反向传播法" class="headerlink" title="5. 误差反向传播法"></a>5. 误差反向传播法</h1><blockquote>
<p>上一节介绍了神经网络的学习，并通过数值微分计算了神经网络的权重参数的梯度（严格地说，是损失函数关于权重参数的梯度）。数值微分简单、容易实现，但是计算很费时间。</p>
</blockquote>
<blockquote>
<p>我们将介绍误差反向传播法，要正确理解它，有两种方法：基于数学式和基于<em>计算图</em>（computational graph）。前者在各大图书中经常出现，但是如果直接从数学式子开始探讨，很容易止步于式子的罗列。本书希望通过计算图来更直观地理解误差反向传播法</p>
</blockquote>
<blockquote>
<p>通过计算图来理解误差反向传播法的想法，参考了 AndrejKarpathy 的博客和他与 Fei-Fei Li 教师复杂的斯坦福大学的深度学习课程 CS231n</p>
</blockquote>
<h2 id="5-1-计算图"><a href="#5-1-计算图" class="headerlink" title="5.1 计算图"></a>5.1 计算图</h2><blockquote>
<p>计算题将计算过程用图形表示出来，这里说的图形是数据结构图，通过多个节点和边表示（连接节点的直线称为‘边’）。</p>
</blockquote>
<h3 id="5-1-1-用计算图求解"><a href="#5-1-1-用计算图求解" class="headerlink" title="5.1.1 用计算图求解"></a>5.1.1 用计算图求解</h3><ul>
<li>问题 1：太郎在超市买了 2 个 100 日元一个的苹果，消费税是 10%，请计算支付金额</li>
</ul>
<blockquote>
<p>计算题通过节点和箭头表示计算过程。节点用〇表示，〇中是计算的内容。将计算的中间结果写在箭头的上方，表示各个节点的计算结果从左向右传递。</p>
</blockquote>
<p><img src="20230831171049.png"></p>
<blockquote>
<p>如图所示，开始时，苹果的 100 日元流到‘x2’节点，变成 200 日元，然后被传递到下一个节点。接着，这个 200 日元流向‘x1.1’节点，变成 220 日元，因此，答案为 220 日元</p>
</blockquote>
<blockquote>
<p>上图吧‘x2’和‘x1.1’等作为一个运算符整体用〇括起来了，不过只用〇表示乘法运算‘x’也是可行的</p>
</blockquote>
<p><img src="20230831171426.png"></p>
<ul>
<li>问题 2：太郎在超市买了 2 个苹果、3 个橘子。其中，苹果每个 100 日元，橘子每个 150 日元。消费税是 10%，请计算支付金额</li>
</ul>
<p><img src="20230831172130.png"></p>
<blockquote>
<p>综上，用计算图解题的情况下，需要按如下流程进行：</p>
</blockquote>
<ol>
<li>构建计算图</li>
<li>在计算图上，从左向右进行计算</li>
</ol>
<blockquote>
<p>‘从左向右进行计算’是一种正方向上的传播，简称为<em>正向传播</em>（forward propagation）。正向传播是从计算图出发点到结束点的传播。自然，也有反方向的传播，_反向传播_（backward propagation），反向传播在接下来的导数计算中发挥重要作用</p>
</blockquote>
<h3 id="5-1-2-局部计算"><a href="#5-1-2-局部计算" class="headerlink" title="5.1.2 局部计算"></a>5.1.2 局部计算</h3><blockquote>
<p>计算图的特征是可以通过传递‘局部计算’获得最终结果。‘局部’是指，‘与自己有关的某个小范围’。局部计算是指，无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。</p>
</blockquote>
<blockquote>
<p>比如在超市里买了两个苹果和其他很多东西</p>
</blockquote>
<p><img src="20230831183950.png"></p>
<blockquote>
<p>假设（经过复杂的计算）购买的其他很多东西总共花费 4000 日元。这里的重点是，各个节点处的计算都是局部计算。也就是说，苹果和其他很多东西的求和计算并不关心 4000 这个数字是怎么来的，只进行求和。换言之，各个节点处只需进行与自己有关的运算，不用考虑全局</p>
</blockquote>
<blockquote>
<p>综上，计算图专注局部计算，各个步骤所要做的就是对象节点的局部计算。通过传递它的计算结果，可以获得全局的复杂计算的结果</p>
</blockquote>
<ul>
<li>组装汽车是一个复杂的工作，通常需要进行‘流水线’作业。每个工人（机器）承担的都是简化了的工作，这个工作的成果会传给下一个工人，直至汽车组装完成。计算图将复杂计算分割成简单的局部计算，和流水线作业一样，将局部计算的结果传递给下一个节点。</li>
</ul>
<h3 id="5-1-3-为何用计算图解题"><a href="#5-1-3-为何用计算图解题" class="headerlink" title="5.1.3 为何用计算图解题"></a>5.1.3 为何用计算图解题</h3><blockquote>
<p>计算图的优点：1. 无论全局计算多么复杂，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题。2. 利用计算图可以将中间的计算结果全部保存起来。3. 最重要的一点，可以通过反向传播高效计算导数。</p>
</blockquote>
<blockquote>
<p>我们思考一下问题 1，假设我们现在的苹果价格的上涨会在多大程度上影响需要支付的金额，即求‘支付金额关于苹果的价格的导数’。设苹果价格为 x，支付金额为 L，则相对于求 əL/əx。这个导数的值表示当苹果价格稍微上涨时，支付金额会增加多少。</p>
</blockquote>
<blockquote>
<p>‘支付金额关于苹果的导数’可以通过计算图的反向传播求出来</p>
</blockquote>
<p><img src="20230831185650.png"></p>
<blockquote>
<p>这里反向箭头的下方是局部导数。结果是 2.2，也就是说，如果苹果的价格上涨 1 日元，则支付金额会增加 2.2 日元</p>
</blockquote>
<blockquote>
<p>‘支付金额关于消费税的导数’，‘支付金额关于苹果的个数的导数’也可以通过相同的方式求出。并且，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值</p>
</blockquote>
<h2 id="5-2-链式法则"><a href="#5-2-链式法则" class="headerlink" title="5.2 链式法则"></a>5.2 链式法则</h2><h3 id="5-2-1-计算图的反向传播"><a href="#5-2-1-计算图的反向传播" class="headerlink" title="5.2.1 计算图的反向传播"></a>5.2.1 计算图的反向传播</h3><blockquote>
<p>假设存在 y=f(x)，反向传播如图所示</p>
</blockquote>
<p><img src="20230831194853.png"></p>
<blockquote>
<p>反向传播的计算顺序是，将信号 E 乘以节点的局部导数(əy/əx)，然后将结果传递给下一个节点。这里的局部导数是指正向传播中 y=f(x)的导数，也就是 y 关于 x 的导数。比如 y=f(x)=x^2，则 əy/əx=2x，把这个导数乘以上游传过来的值（E），然后传递给前面的节点。</p>
</blockquote>
<blockquote>
<p>这就是反向传播的计算顺序。通过这种计算，可以高效求出导数的值，这是反向传播的要点。这是如何实现的？可以通过链式法则的原理进行解释。</p>
</blockquote>
<h3 id="5-2-2-什么是链式法则"><a href="#5-2-2-什么是链式法则" class="headerlink" title="5.2.2 什么是链式法则"></a>5.2.2 什么是链式法则</h3><blockquote>
<p>先从<em>复合函数</em>说起，复合函数是由多个函数构成的函数。比如 z=(x+y)^2</p>
</blockquote>
<p><img src="20230831195610.png"></p>
<blockquote>
<p>链式法则是关于复合函数的导数的性质</p>
</blockquote>
<pre><code>如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。
</code></pre>
<p><img src="20230831200435.png"></p>
<h3 id="5-2-3-链式法则和计算图"><a href="#5-2-3-链式法则和计算图" class="headerlink" title="5.2.3 链式法则和计算图"></a>5.2.3 链式法则和计算图</h3><p><img src="20230831200548.png"><br><img src="20230831200754.png"><br><img src="20230831200856.png"></p>
<h2 id="5-3-反向传播"><a href="#5-3-反向传播" class="headerlink" title="5.3 反向传播"></a>5.3 反向传播</h2><blockquote>
<p>本节以‘+’和‘x’等计算为例，介绍反向传播的结构。</p>
</blockquote>
<h3 id="5-3-1-加法节点的反向传播"><a href="#5-3-1-加法节点的反向传播" class="headerlink" title="5.3.1 加法节点的反向传播"></a>5.3.1 加法节点的反向传播</h3><blockquote>
<p>这里以 z=x+y 为对象，观察它的反向传播</p>
</blockquote>
<p><img src="20230831202343.png"></p>
<blockquote>
<p>z=x+y 的导数可以由上面这两个式子计算出来。此时，两式都等于 1。在下图中，反向传播将从上游传过来的导数乘以 1，然后传向下游。因为加法节点的反向传播只乘以 1，所以输入的值会原封不动地流向下一个节点。</p>
</blockquote>
<p><img src="20230831202606.png"></p>
<blockquote>
<p>另外，本例将上游传来的导数值设为 əL/əx。这是因为，我们假定最终输出值为 L 的大型计算图。z=x+y 的计算位于其中，从上游会传来 əL/əz 的值，并向下游传递 əL/əx 和 əL/əy</p>
</blockquote>
<p><img src="20230831202816.png"><br><img src="20230831202928.png"></p>
<h3 id="5-3-2-乘法节点的反向传播"><a href="#5-3-2-乘法节点的反向传播" class="headerlink" title="5.3.2 乘法节点的反向传播"></a>5.3.2 乘法节点的反向传播</h3><blockquote>
<p>考虑 z=xy</p>
</blockquote>
<p><img src="20230831210742.png"></p>
<blockquote>
<p>乘法的反向传播会将上游的值乘以正向传播时的输入信号的‘翻转值’后传递给下游。翻转值表示一种翻转关系，如下图 5-12，正向传播时信号是 x 的话，反向传播时则是 y；正向传播时的信号是 y 的话，反向传播时则是 x。</p>
</blockquote>
<p><img src="20230831211007.png"></p>
<blockquote>
<p>乘法的反向传播会乘以输入信号的翻转值，而加法的反向传播只是将上游的值传给下游，并不需要正向传播的输入信号。但是，乘法的反向传播需要正向传播时的输入信号值。因此，实现乘法节点的反向传播时，要保证正向传播的输入信号</p>
</blockquote>
<h3 id="5-3-3-苹果的例子"><a href="#5-3-3-苹果的例子" class="headerlink" title="5.3.3 苹果的例子"></a>5.3.3 苹果的例子</h3><blockquote>
<p>最开始苹果的例子（2 个苹果和消费税）。这里要解的问题是苹果的价格、苹果的个数、消费税这 3 个变量各自如何影响最终支付的金额。相对于求‘支付金额关于苹果的价格/苹果的个数/消费税的导数’</p>
</blockquote>
<p><img src="20230831212023.png"></p>
<blockquote>
<p>乘法节点的反向传播会将输入信号翻转后传给下游。从图 5-14 可知，苹果的价格的导数是 2.2，苹果的个数的导数是 110，消费税的导数是 200。这可以解释为，如果消费税和苹果的价格增加相同的值，则消费税将对最终价格产生 200 倍大小的影响，苹果的价格将产生 2.2 倍大小的影响。不过，这个例子里消费税和苹果的价格的量纲不同，所以才形成了这样的结果（消费税的 1 是 100%，苹果的价格的 1 是 1 日元。</p>
</blockquote>
<p><img src="20230831212930.png" alt="小练习"></p>
<h2 id="5-4-简单层的实现"><a href="#5-4-简单层的实现" class="headerlink" title="5.4 简单层的实现"></a>5.4 简单层的实现</h2><blockquote>
<p>我们将要实现的计算图的乘法节点称为‘乘法层’（MulLayer），加法节点称为‘加法层’（AddLayer）</p>
</blockquote>
<ul>
<li>下一节中，我们将把构建神经网络的‘层’实现为一个类。这里说的‘层’是神经网络中功能的单位。比如，负责 sigmoid 函数的 sigmoid、负责矩阵乘积的 Affine 等，都以层为单位实现。</li>
</ul>
<h3 id="5-4-1-乘法层的实现"><a href="#5-4-1-乘法层的实现" class="headerlink" title="5.4.1 乘法层的实现"></a>5.4.1 乘法层的实现</h3><blockquote>
<p>层的实现中有两个共通的方法（接口）forward()和 backward()。forward 对应正向传播，backward 对应反向传播</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MulLayer</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> None
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> None

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> y
        out <span class="token operator">=</span> x <span class="token operator">*</span> y

        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>y  <span class="token comment" spellcheck="true"># 翻转x和y</span>
        dy <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>x

        <span class="token keyword">return</span> dx<span class="token punctuation">,</span>dy
</code></pre>
<blockquote>
<p>init 方法中初始化实例变量 x 和 y，它们用于保存正向传播时的输入值。forward 接收 x 和 y 两个参数，将它们相乘后输出。backward 将从上游传来的导数（dout）乘以正向传播的翻转值，然后传给下游。</p>
</blockquote>
<p><img src="20230831215636.png"></p>
<blockquote>
<p>使用这个乘法层实现上图的正向传播</p>
</blockquote>
<pre class=" language-python"><code class="language-python">apple <span class="token operator">=</span> <span class="token number">100</span>
apple_num <span class="token operator">=</span> <span class="token number">2</span>
tax <span class="token operator">=</span> <span class="token number">1.1</span>

<span class="token comment" spellcheck="true"># layer</span>
mul_apple_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
mul_tex_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># forward</span>
apple_price <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple<span class="token punctuation">,</span> apple_num<span class="token punctuation">)</span>
price <span class="token operator">=</span> mul_tex_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple_price<span class="token punctuation">,</span> tax<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 220.00000000000003</span>

<span class="token comment" spellcheck="true"># 各个变量的导数</span>
<span class="token comment" spellcheck="true"># backward</span>
dprice <span class="token operator">=</span> <span class="token number">1</span>
dapple_price<span class="token punctuation">,</span> dtax <span class="token operator">=</span> mul_tex_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dprice<span class="token punctuation">)</span>
dapple<span class="token punctuation">,</span> dapple_num <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dapple_price<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 2.2 110.00000000000001 200</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dapple<span class="token punctuation">,</span> dapple_num<span class="token punctuation">,</span> dtax<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>调用 backward 的顺序和调用 forward 的顺序相反。此外，backward 的参数中需要输入‘关于正向传播时的输出变量的导数’。比如，mul_apple_layer 乘法层在正向传播时会输出 apple_price，在反向传播时，则会将 apple_price 的导数 dapple_price 设为参数。</p>
</blockquote>
<h3 id="5-4-2-加法层的实现"><a href="#5-4-2-加法层的实现" class="headerlink" title="5.4.2 加法层的实现"></a>5.4.2 加法层的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AddLayer</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> x <span class="token operator">+</span> y
        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span>
        dy <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span>
        <span class="token keyword">return</span> dx<span class="token punctuation">,</span> dy
</code></pre>
<blockquote>
<p>现在，我们使用乘法层和加法层实现图 5-17 购买 2 个苹果和 3 个橘子的例子</p>
</blockquote>
<p><img src="20230831220736.png"></p>
<pre class=" language-python"><code class="language-python">apple <span class="token operator">=</span> <span class="token number">100</span>
apple_num <span class="token operator">=</span> <span class="token number">2</span>
orange <span class="token operator">=</span> <span class="token number">150</span>
orange_num <span class="token operator">=</span> <span class="token number">3</span>
tax <span class="token operator">=</span> <span class="token number">1.1</span>

<span class="token comment" spellcheck="true"># layer</span>
mul_apple_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
mul_orange_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
add_apple_orange_layer <span class="token operator">=</span> AddLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
mul_tax_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># forward</span>
apple_price <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple<span class="token punctuation">,</span> apple_num<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (1)</span>
orange_price <span class="token operator">=</span> mul_orange_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>orange<span class="token punctuation">,</span> orange_num<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (2)</span>
all_price <span class="token operator">=</span> add_apple_orange_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple_price<span class="token punctuation">,</span> orange_price<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (3)</span>
price <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>all_price<span class="token punctuation">,</span> tax<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (4)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>apple_price<span class="token punctuation">,</span>orange_price<span class="token punctuation">,</span>all_price<span class="token punctuation">,</span>price<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 200 450 650 715</span>

<span class="token comment" spellcheck="true"># backward</span>
dprice <span class="token operator">=</span> <span class="token number">1</span>
dall_price<span class="token punctuation">,</span> dtax <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dprice<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (4)</span>
dapple_price<span class="token punctuation">,</span> dorange_price <span class="token operator">=</span> add_apple_orange_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dall_price<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (3)</span>
dorange<span class="token punctuation">,</span> dorange_num <span class="token operator">=</span> mul_orange_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dorange_price<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (2)</span>
dapple<span class="token punctuation">,</span> dapple_num <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dapple_price<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (1)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 715</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dall_price<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 715</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dapple_num<span class="token punctuation">,</span> dapple<span class="token punctuation">,</span> dorange<span class="token punctuation">,</span> dorange_num<span class="token punctuation">,</span> dtax<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 110 2.2 3.3 165 650</span>
</code></pre>
<h2 id="5-5-激活函数层的实现"><a href="#5-5-激活函数层的实现" class="headerlink" title="5.5 激活函数层的实现"></a>5.5 激活函数层的实现</h2><blockquote>
<p>我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先实现激活函数的 ReLU 层和 Sigmoid 层</p>
</blockquote>
<h3 id="5-5-1-ReLU-层"><a href="#5-5-1-ReLU-层" class="headerlink" title="5.5.1 ReLU 层"></a>5.5.1 ReLU 层</h3><p><img src="20230831222214.png"></p>
<blockquote>
<p>在式子（5.8）中，如果正向传播的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时 x 小于等于 0，则反向传播中传给下游的信号将停在此处。实现 ReLU 层，其中，假定传入 forward 和 backward 的参数是 numpy 数组</p>
</blockquote>
<p><img src="20230901212906.png"></p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Relu</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> None

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> x<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 小于0的都设为0</span>
        out<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dout<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        dx <span class="token operator">=</span> dout

        <span class="token keyword">return</span> dx


<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment" spellcheck="true"># mask</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [[ 1.  -0.5]</span>
<span class="token comment" spellcheck="true">#  [-2.   3. ]]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>

mask <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [[False  True]</span>
<span class="token comment" spellcheck="true">#  [ True False]]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>Relu 类有实例变量 mask。这个变量是 numpy 布尔数组，它会把正向传播时的输入 x 的元素中小于等于 0 的地方保存为 True，大于零的保存为 False。</p>
</blockquote>
<blockquote>
<p>如果正向传播时的输入值小于等于 0，则反向传播的值为 0。因此，反向传播中会使用正向传播时保存的 mask，将从上游传来的 dout 的 mask 中的元素为 True 的地方设为 0。</p>
</blockquote>
<ul>
<li>ReLU 层的作用就像电路开关。正向传播时，有电流通过的话，就将开关设为 ON；没有就设为 OFF。反向传播时，开关为 ON，电流会直接通过；开关为 OFF 则不会有电流通过</li>
</ul>
<h3 id="5-5-2-Sigmoid-层"><a href="#5-5-2-Sigmoid-层" class="headerlink" title="5.5.2 Sigmoid 层"></a>5.5.2 Sigmoid 层</h3><p><img src="20230901214117.png"></p>
<blockquote>
<p>反向传播步骤</p>
</blockquote>
<p><img src="20230901214720.png"><br><img src="20230901214749.png"></p>
<blockquote>
<p>反向传播的输出为 əL/əy * y^2 * exp(-x)，这个值会传播给下游的节点。这个结果只根据正向传播时的输入 x 和输出 y 就可以算出来，可以将图 5-20 画成下面的集约化的 sigmoid 节点。</p>
</blockquote>
<p><img src="20230901214945.png"></p>
<blockquote>
<p>简洁版的计算图可以省略反向传播中的计算过程，计算效率更高。此外，通过对节点进行集约化，可以不用在意 sigmoid 层中琐碎的细节，而只需要专注它的输入输出。</p>
</blockquote>
<p><img src="20230901215046.png"><br><img src="20230901215123.png"></p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Sigmoid</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> None

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> out

        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>out<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>out

        <span class="token keyword">return</span> dx
</code></pre>
<h2 id="5-6-Affine-Softmax-层的实现"><a href="#5-6-Affine-Softmax-层的实现" class="headerlink" title="5.6 Affine/Softmax 层的实现"></a>5.6 Affine/Softmax 层的实现</h2><h3 id="5-6-1-Affine-层"><a href="#5-6-1-Affine-层" class="headerlink" title="5.6.1 Affine 层"></a>5.6.1 Affine 层</h3><blockquote>
<p>神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算（numpy 中的 np.dot()）</p>
</blockquote>
<pre class=" language-python"><code class="language-python">X <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 输入</span>
W <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 输入</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 输入</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (2,)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>W<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (2, 3)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># (3,)</span>

Y <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">+</span> B
</code></pre>
<blockquote>
<p>神经元的加权和可以用 Y=np.dot(X,W)+B 计算。然后，Y 经过激活函数转换后，传递给下一层。这就是神经网络正向传播的流程。此外，矩阵乘法计算的要点是使对应维度的元素个数一致。</p>
</blockquote>
<p><img src="20230901220450.png"></p>
<ul>
<li>神经网络的正向传播中进行的矩阵乘法计算在几何领域称为‘仿射变换’。（几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算）。因此，这里将进行仿射变换的处理实现为‘Affine 层’</li>
</ul>
<blockquote>
<p>将求矩阵的乘积与偏置的和的运算用计算图表示出来。</p>
</blockquote>
<p><img src="20230901220806.png"></p>
<blockquote>
<p>这里，X、W、B 是矩阵（多维数组），而之前的计算图中各个节点间流动的是标量。</p>
</blockquote>
<blockquote>
<p>以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同</p>
</blockquote>
<p><img src="20230901221016.png"></p>
<blockquote>
<p>W^T 表示 W 的转置。上面的式子用数学式表示：</p>
</blockquote>
<p><img src="20230901221022.png"><br><img src="20230901221304.png"><br><img src="20230901221545.png"></p>
<h3 id="5-6-2-批处理版本的-Affine-层"><a href="#5-6-2-批处理版本的-Affine-层" class="headerlink" title="5.6.2 批处理版本的 Affine 层"></a>5.6.2 批处理版本的 Affine 层</h3><blockquote>
<p>现在考虑 N 个数据一起进行正向传播的情况</p>
</blockquote>
<p><img src="20230908214906.png"></p>
<blockquote>
<p>输入 X 的形状是(N,2)</p>
</blockquote>
<blockquote>
<p>正向传播时，偏置被加到 X·W 的各个数据上，设 N=2</p>
</blockquote>
<pre class=" language-python"><code class="language-python">X_dot_W <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [[ 1  2  3]</span>
<span class="token comment" spellcheck="true">#  [11 12 13]]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X_dot_W <span class="token operator">+</span> B<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>正向传播时，偏置加到每一个数据上，因此，反向传播时，各个数据的反向传播的值需要汇总为偏置的元素</p>
</blockquote>
<pre class=" language-python"><code class="language-python">dY <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
dB <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dY<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [5 7 9]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dB<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>假定数据有 2 个，偏置的反向传播会对这 2 个元素的导数按元素进行求和。</p>
</blockquote>
<blockquote>
<p>下面是 Affine 函数的实现，common/layers.py 中的函数考虑到输入为张量（四维数据）的的情况，所以有所不同</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Affine</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__int__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> W
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> b
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> None
        self<span class="token punctuation">.</span>dW <span class="token operator">=</span> None
        self<span class="token punctuation">.</span>db <span class="token operator">=</span> None

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x
        out <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b

        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dout<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">.</span>T<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dW <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token punctuation">.</span>T<span class="token punctuation">,</span> dout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>db <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dout<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> dx
</code></pre>
<h3 id="5-6-3-Softmax-with-Loss-层"><a href="#5-6-3-Softmax-with-Loss-层" class="headerlink" title="5.6.3 Softmax-with-Loss 层"></a>5.6.3 Softmax-with-Loss 层</h3><blockquote>
<p>softmax 函数会将输入值正规化后再输出。比如手写数字识别时，softmax 层的输出如下</p>
</blockquote>
<p><img src="20230908220441.png"></p>
<blockquote>
<p>图中，softmax 层将输入值正规化（将输出值的和调整为 1）之后再输出。另外，因为手写数字识别要 10 分类，所以向 softmax 层的输入也有 10 个。</p>
</blockquote>
<ul>
<li>神经网络中有<em>推理</em>（inference）和<em>学习</em>两个阶段。神经网络的推理通常不使用 Softmax 层，比如刚才那个例子，会将 Affine 层的输出作为识别结果。神经网络中未被正规化的输出结果有时称为‘得分’，也就是说，神经网络的推理只需要一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax 层。不过，神经网络的学习阶段需要 Softmax 层。</li>
</ul>
<blockquote>
<p>下面实现 Softmax 层。这里也包含作为损失函数的交叉熵误差（cross entropy error），所以称为‘Softmax-with-Loss’层</p>
</blockquote>
<p><img src="20230908221034.png"></p>
<blockquote>
<p>这里假设要进行 3 分类，Softmax 层从前面的层接收 3 个输入（得分）。将输入(a1,a2,a3)正规化，输出(y1,y2,y3)。Cross Entropy Error 层接收 Softmax 层的输出(y1,y2,y3)和监督标签(t1,t2,t3)，从这些数据中输出损失 L</p>
</blockquote>
<p><img src="20230908221317.png"></p>
<blockquote>
<p>注意图中反向传播的结果。Softmax 层的反向传播得到了(y1-t1, y2-t2, y3-t3)这样‘漂亮’的结果，(y1, y2, y3)是 Softmax 层的输出，(t1, t2, t3)是监督数据，所以该结果表示 Softmax 层的输出和监督标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。</p>
</blockquote>
<blockquote>
<p>神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax 的输出）接近监督标签。因此，必须将神经网络的输出和监督标签的误差高效地传递给前面的层。(y1-t1, y2-t2, y3-t3)则直截了当地表示了当前神经网络的输出与监督标签的误差。</p>
</blockquote>
<blockquote>
<p>假设监督标签是(0,1,0)，Softmax 层输出是(0.3,0.2,0.5)，因为正确解标签处的概率是 0.2（20%），这个时候神经网络未能正确识别。此时，Softmax 层的反向传播传递的是(0.3,-0.8,0.5)这样一个大误差。并传给前面的层，它们将从中学习‘大’的内容。</p>
</blockquote>
<ul>
<li>使用交叉熵误差作为 softmax 函数的损失函数得到漂亮的输出结果不是偶然，而是为了得到这样的结果特意设计的交叉熵误差函数。同理，回归问题中输出层使用‘恒等函数’，损失函数使用‘平方和误差’也是出于同样的理由。</li>
</ul>
<blockquote>
<p>假设监督标签(0,1,0)，Softmax 层的输出是(0.01,0.99,0)，此时，Softmax 层反向传播传递的是(0.01,-0.01,0)，这样小的误差，前面的层也学习到‘小’的内容。</p>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SoftmaxWithLoss</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>loss <span class="token operator">=</span> None  <span class="token comment" spellcheck="true"># 损失</span>
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> None  <span class="token comment" spellcheck="true"># softmax的输出</span>
        self<span class="token punctuation">.</span>t <span class="token operator">=</span> None  <span class="token comment" spellcheck="true"># 监督数据(ont-hot vector)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>t <span class="token operator">=</span> t
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 3.5.2 实现</span>
        self<span class="token punctuation">.</span>loss <span class="token operator">=</span> cross_entropy_error<span class="token punctuation">(</span>self<span class="token punctuation">.</span>y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>t<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 4.2.4 实现</span>

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_size <span class="token operator">=</span> self<span class="token punctuation">.</span>t<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        dx <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>y <span class="token operator">-</span> self<span class="token punctuation">.</span>t<span class="token punctuation">)</span> <span class="token operator">/</span> batch_size

        <span class="token keyword">return</span> dx
</code></pre>
<blockquote>
<p>注意这里的反向传播的值，将要传播的值除以批大小(batch_size)后，传递给前面的是单个数据的误差</p>
</blockquote>
<h2 id="5-7-误差反向传播法的实现"><a href="#5-7-误差反向传播法的实现" class="headerlink" title="5.7 误差反向传播法的实现"></a>5.7 误差反向传播法的实现</h2><h3 id="5-7-1-神经网络学习的全貌图"><a href="#5-7-1-神经网络学习的全貌图" class="headerlink" title="5.7.1 神经网络学习的全貌图"></a>5.7.1 神经网络学习的全貌图</h3><ul>
<li>前提<br>神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。分为以下 4 个步骤</li>
<li>步骤 1（mini-batch）<br>从训练数据中随机选择一部分数据</li>
<li>步骤 2（计算梯度）<br>计算损失函数关于各个权重参数的梯度</li>
<li>步骤 3（更新参数）<br>将权重参数沿梯度方向进行微小的更新</li>
<li>步骤 4（重复）<br>重复步骤 1、2、3</li>
</ul>
<blockquote>
<p>之前，我们通过数值微分来计算梯度，数值微分实现简单，但是计算需要耗费很多时间，而误差反向传播法可以快速高效地计算梯度</p>
</blockquote>
<h3 id="5-7-2-对应误差反向传播法的神经网络的实现"><a href="#5-7-2-对应误差反向传播法的神经网络的实现" class="headerlink" title="5.7.2 对应误差反向传播法的神经网络的实现"></a>5.7.2 对应误差反向传播法的神经网络的实现</h3><p><img src="20230908224743.png"></p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>pardir<span class="token punctuation">)</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>common<span class="token punctuation">.</span>layers <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>common<span class="token punctuation">.</span>gradient <span class="token keyword">import</span> numerical_gradient
<span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict


<span class="token keyword">class</span> <span class="token class-name">TwoLayerNet</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span>
                 weight_init_std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 初始化权重</span>
        self<span class="token punctuation">.</span>params <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> \
                            np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> \
                            np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>output_size<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># 生成层</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> \
            Affine<span class="token punctuation">(</span>self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Relu1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> Relu<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> \
            Affine<span class="token punctuation">(</span>self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lastLayer <span class="token operator">=</span> SoftmaxWithLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x

    <span class="token comment" spellcheck="true"># x: 输入数据，t: 监督数据</span>
    <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>lastLayer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">accuracy</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 预测结果（概率最大的列）</span>
        y <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> t<span class="token punctuation">.</span>ndim <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">:</span> t <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        accuracy <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>y <span class="token operator">==</span> t<span class="token punctuation">)</span> <span class="token operator">/</span> float<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> accuracy

    <span class="token comment" spellcheck="true"># x: 输入数据，t: 监督数据</span>
    <span class="token keyword">def</span> <span class="token function">numerical_gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss_W <span class="token operator">=</span> <span class="token keyword">lambda</span> W<span class="token punctuation">:</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

        grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        grads<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> grads

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># forward</span>
        self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># backward</span>
        dout <span class="token operator">=</span> <span class="token number">1</span>
        dout <span class="token operator">=</span> self<span class="token punctuation">.</span>lastLayer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>

        layers <span class="token operator">=</span> list<span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        layers<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> layers<span class="token punctuation">:</span>
            dout <span class="token operator">=</span> layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># 设定</span>
        grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        grads<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dW
        grads<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>db
        grads<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dW
        grads<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>db

        <span class="token keyword">return</span> grads
</code></pre>
<blockquote>
<p>将神经网络的层保存为有序字典很重要，这样一来，神经网络的正向传播只需要按照添加元素的顺序调用各层的 forward 方法，而反向传播则按照相反的顺序调用各层即可。像这样模块化地构建神经网络，非常简单快捷。</p>
</blockquote>

            </div>
            <hr/>

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff;
        background-color: #22AB38;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff;
        background-color: #019FE8;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a class="reward-link btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs">
                        <li class="tab wechat-tab waves-effect waves-light"><a class="active" href="#wechat">微信</a></li>
                        <li class="tab alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                    </ul>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.png" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('#reward .reward-link').on('click', function () {
            $('#rewardModal').openModal();
        });

        $('#rewardModal .close').on('click', function () {
            $('#rewardModal').closeModal();
        });
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            <div class="reprint">
                <p>
                    <span class="reprint-tip">
                        <i class="fa fa-exclamation-circle"></i>&nbsp;&nbsp;转载请注明:
                    </span>
                    <a href="https://malred.github.io" class="b-link-green">malred-blog</a>
                    <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                    <a href="/2023/08/26/ji-suan-ji-li-lun-ji-shu-ji/shi-ti/shen-du-xue-xi-ru-men-ji-yu-python-de-li-lun-he-shi-xian/z/" class="b-link-green">深度学习入门-基于python的理论与实现</a>
                </p>
            </div>
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '676b06df8b7379e50e35',
        clientSecret: '9159b4de0f716ba7bdab9bc02151a90e6961246b',
        repo: 'comment_repo',
        owner: 'malred',
        admin: "malred",
        id: '2023-08-26T05-50-15',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    
        <link rel="stylesheet" href="/libs/gitment/gitment-default.css">
<link rel="stylesheet" href="/css/gitment.css">

<div class="gitment-card card" data-aos="fade-up">
    <div id="gitment-content" class="card-content"></div>
</div>

<script src="/libs/gitment/gitment.js"></script>
<script>
var gitment = new Gitment({
    id: 'Sat Aug 26 2023 05:50:15 GMT+0800',
    owner: 'malred',
    repo: 'comment_repo',
    oauth: {
        client_id: '676b06df8b7379e50e35',
        client_secret: '9159b4de0f716ba7bdab9bc02151a90e6961246b'
    }
});

gitment.render('gitment-content');
</script>
    

    
        <div class="disqus-card card" data-aos="fade-up">
    <div id="disqus_thread" class="card-content">
        <noscript>Please enable JavaScript to view the
            <a target="_blank" rel="noopener" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
    </div>
</div>

<script type="text/javascript">
    disqus_config = function () {
        this.page.url = 'https://malred.github.io/2023/08/26/ji-suan-ji-li-lun-ji-shu-ji/shi-ti/shen-du-xue-xi-ru-men-ji-yu-python-de-li-lun-he-shi-xian/z/';
        this.page.identifier = '/2023/08/26/ji-suan-ji-li-lun-ji-shu-ji/shi-ti/shen-du-xue-xi-ru-men-ji-yu-python-de-li-lun-he-shi-xian/z/';
        this.page.title = '深度学习入门-基于python的理论与实现';
    };
    let disqus_shortname = '';

    (function () { // DON'T EDIT BELOW THIS LINE
        let d = document, s = d.createElement('script');
        // 如：s.src = 'https://blinkfox.disqus.com/embed.js';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments input[type=text],
    #vcomments input[type=email],
    #vcomments input[type=url],
    #vcomments textarea {
        box-sizing: border-box;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #42b983;
        font-weight: 500;
        text-decoration: underline;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div id="vcomments" class="card-content"></div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'mipjlux8dRwbE7yY1zP3v13z-gzGzoHsz',
        appKey: 'cIy0pwvRAK8cuOCZRjuoHHfy',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: '留下友善的评论~'
    });
</script>
    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2023/08/28/backend/java/lg2088-qu-xue-she-ji-mo-shi-la-gou-zhuan-lan/z/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/23.jpg" class="responsive-img" alt="lg2088_趣学设计模式-拉钩专栏">
                        
                        <span class="card-title">lg2088_趣学设计模式-拉钩专栏</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">01 学习思维：怎样才能学好 Java 设计模式？_6862
02 组合思维：Unix 哲学到底给现代编程带来哪些重要启示？
</div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2023-08-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" class="post-category" target="_blank">
                                    设计模式
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" target="_blank">
                        <span class="chip bg-color">设计模式</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/08/23/ai/shen-du-zhi-yan-kaggle/z/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/11.jpg" class="responsive-img" alt="【2023.04】SDZY-AI大赛年度视频-无课件（28.08G）">
                        
                        <span class="card-title">【2023.04】SDZY-AI大赛年度视频-无课件（28.08G）</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">打造舒适的 AI 开发环境01-【kaggle 新赛】酶稳定性预测大赛05-【01 课】赛题介绍 + Kaggle 平台学习 + 开发环境搭建 + 比赛数据探索性分析_ev

8 步进行建模



kaggle



比赛



比赛目的
</div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2023-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/ai/" class="post-category" target="_blank">
                                    ai
                                </a>
                            
                            <a href="/categories/ai/kaggle/" class="post-category" target="_blank">
                                    kaggle
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/ai/" target="_blank">
                        <span class="chip bg-color">ai</span>
                    </a>
                    
                    <a href="/tags/kaggle/" target="_blank">
                        <span class="chip bg-color">kaggle</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


    </div>
    <div class="col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });
    });
</script>
    

</main>


<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            本站由&copy;<a href="https://blinkfox.github.io/" target="_blank">Blinkfox</a>基于
            <a href="https://hexo.io/" target="_blank">Hexo</a> 的
            <a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">hexo-theme-matery</a>主题搭建.

            
                &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
                <span class="white-color">608.6k</span>
            

            
			
                <br>
                
                <span id="busuanzi_container_site_pv">
                    <i class="fa fa-heart-o"></i>
                    本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span>
                </span>
                
                
                <span id="busuanzi_container_site_uv">
                    <i class="fa fa-users"></i>
                    次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人.
                </span>
                
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/malred" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:malguy2022@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2725953379" class="tooltipped" data-tooltip="QQ联系我: 2725953379" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>


</div>
    </div>
</footer>

<div class="progress-bar"></div>


<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input" autofocus="">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/js/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<!--动态线条背景-->
<script type="text/javascript" color="122 103 238" opacity='0.7' zIndex="-2" count="200"
    src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
<script src="/js/matery.js"></script>
<script src="/js/snow.js"></script>

<!-- 引用Aplayer和metingjs -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@1.2.0/dist/Meting.min.js"></script>
<div id="my-aplayer" class="aplayer" data-id="7363940489" data-server="netease" data-type="playlist" data-fixed="true"
    // 吸底模式可以固定播放器于页面底部 data-autoplay="true" data-order="list" data-volume="0.55" data-theme="#cc543a"
    data-preload="auto"></div>  
<!-- <script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script> -->
<!-- <script src="https://cdn.bootcss.com/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script>
    // 对所有链接跳转事件绑定pjax容器container
    $(document).pjax('a[target!=_blank]', '.container', { fragment: '.container', timeout: 8000 })
</script> -->


<!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id=G-MSEYRC5EW5"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'G-MSEYRC5EW5');
</script>



    <script src="/libs/others/clicklove.js"></script>


    <script async src="/libs/others/busuanzi.pure.mini.js"></script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>